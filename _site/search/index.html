<!DOCTYPE html>
<html lang="en" class="no-js">
  <!-- Copyright 2019-2021 Vanessa Sochat-->
  <head>
<meta charset="utf-8">
<!-- Copyright 2019-2021 Vanessa Sochat-->

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.55.6" />

<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">

<link rel="alternate" type="application/rss&#43;xml" href="/docs/index.xml">

<link rel="shortcut icon" href="/cloud-ops/assets/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/cloud-ops/assets/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/cloud-ops/assets/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/cloud-ops/assets/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/cloud-ops/assets/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/cloud-ops/assets/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/cloud-ops/assets/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/cloud-ops/assets/favicons/android-96x196.png" sizes="96x196">
<link rel="icon" type="image/png" href="/cloud-ops/assets/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/cloud-ops/assets/favicons/android-192x192.png"sizes="192x192">

<title>Search</title>
<meta property="og:title" content="Search" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:4000" />
<meta property="og:site_name" content="http://localhost:4000" />

<meta itemprop="name" content="Search">
<meta itemprop="description" content="">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Search"/>
<meta name="twitter:description" content=""/>

<link rel="stylesheet" href="/cloud-ops/assets/css/main.css">
<link rel="stylesheet" href="/cloud-ops/assets/css/palette.css">
<script
  src="/cloud-ops/assets/js/jquery-3.3.1/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>
</head>

  

  <body class="td-section">
    <header>
        <nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
        <a class="navbar-brand" href="/cloud-ops/">
            <span class="navbar-logo"></span><svg width="50" height="412" viewBox="0 0 463 412" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M10.6163 388.655C7.52386 388.167 4.73368 387.446 2.19927 386.539V375.797C8.19815 377.75 13.8715 378.704 19.2426 378.704C23.9162 378.704 27.3806 377.797 29.6825 375.983C31.9612 374.169 33.1005 371.542 33.1005 368.054C33.1005 366.101 32.7517 364.474 32.0542 363.125C31.3566 361.776 30.1941 360.591 28.5665 359.544C26.9389 358.498 24.73 357.498 21.8933 356.545L17.336 355.01C6.19852 351.22 0.618164 344.152 0.618164 333.851C0.618164 329.248 1.61798 325.295 3.64086 321.993C5.66374 318.691 8.66318 316.157 12.6624 314.39C16.6617 312.646 21.6143 311.762 27.4736 311.762C30.1243 311.762 32.775 311.925 35.3791 312.274C37.9833 312.623 40.2852 313.088 42.2616 313.716V324.411C38.1461 323.109 33.5655 322.458 28.5432 322.458C18.3823 322.458 13.2902 325.946 13.2902 332.898C13.2902 334.781 13.6157 336.339 14.2435 337.595C14.8713 338.85 15.9409 339.99 17.4057 340.966C18.8938 341.966 20.94 342.896 23.5674 343.78L28.1247 345.314C33.9143 347.267 38.3088 349.965 41.3083 353.429C44.2845 356.894 45.7958 361.404 45.7958 366.961C45.7958 371.635 44.7728 375.658 42.7034 379.006C40.634 382.354 37.6345 384.935 33.6818 386.725C29.729 388.516 24.9625 389.399 19.4054 389.399C16.6384 389.399 13.7088 389.167 10.6163 388.655Z" fill="#1B1A21"/>
<path d="M54.2357 325.156C53.0964 324.086 52.5383 322.575 52.5383 320.622C52.5383 318.669 53.0964 317.157 54.2357 316.064C55.375 314.995 57.0491 314.437 59.258 314.437C61.4669 314.437 63.1643 314.995 64.3036 316.088C65.4429 317.18 66.0242 318.692 66.0242 320.622C66.0242 322.575 65.4662 324.086 64.3268 325.156C63.1875 326.225 61.5134 326.76 59.258 326.76C57.0491 326.76 55.375 326.225 54.2357 325.156Z" fill="#1B1A21"/>
<path d="M65.9085 332.62H53.469V388.423H65.9085V332.62Z" fill="#1B1A21"/>
<path d="M76.9285 332.619H87.2521L88.1822 338.595H88.8565C90.6236 336.386 92.7162 334.689 95.1111 333.503C97.506 332.317 100.157 331.736 103.063 331.736C107.806 331.736 111.573 333.224 114.34 336.2C117.107 339.176 118.502 343.989 118.502 350.593V388.4H106.132V351.546C106.132 348.407 105.505 346.175 104.249 344.85C102.993 343.524 101.203 342.85 98.8779 342.85C97.1108 342.85 95.3204 343.292 93.5533 344.152C91.7862 345.012 90.3678 346.338 89.2983 348.128V388.446H76.9285V332.619Z" fill="#1B1A21"/>
<path d="M143.987 402.002C148.73 402.002 152.218 400.909 154.473 398.723C156.729 396.538 157.845 392.864 157.845 387.679V382.773H157.24C156.054 384.726 154.404 386.26 152.334 387.423C150.265 388.586 147.893 389.167 145.219 389.167C141.871 389.167 138.825 388.237 136.058 386.377C133.291 384.517 131.059 381.517 129.385 377.402C127.688 373.286 126.851 368.008 126.851 361.591C126.851 351.639 129.083 344.175 133.524 339.246C137.965 334.293 144.708 331.805 153.753 331.782C156.543 331.782 159.426 332.015 162.402 332.503C165.378 332.991 167.982 333.619 170.238 334.456V385.656C170.238 394.887 168.145 401.606 164.006 405.768C159.844 409.93 153.52 412 145.033 412C144.591 412 144.429 412 143.987 411.977V402.002ZM154.52 377.657C155.938 376.96 157.054 375.983 157.868 374.774V342.036C156.171 341.664 154.45 341.478 152.753 341.478C148.358 341.478 145.056 342.92 142.848 345.826C140.639 348.733 139.546 353.592 139.546 360.451C139.546 365.218 139.964 368.961 140.825 371.635C141.685 374.309 142.871 376.169 144.382 377.192C145.894 378.215 147.823 378.727 150.149 378.727C151.637 378.727 153.102 378.378 154.52 377.657Z" fill="#1B1A21"/>
<path d="M181.282 388.446V305.764H193.652V388.423H181.282V388.446Z" fill="#1B1A21"/>
<path d="M244.154 364.497H214.95C215.09 368.287 215.648 371.24 216.648 373.403C217.648 375.542 219.159 377.076 221.205 378.006C223.251 378.936 225.972 379.378 229.39 379.378C232.901 379.378 236.97 378.727 241.573 377.425V387.493C239.085 388.167 236.76 388.679 234.598 388.981C232.436 389.283 230.203 389.446 227.925 389.446C221.996 389.446 217.159 388.469 213.416 386.493C209.672 384.517 206.905 381.424 205.092 377.193C203.278 372.937 202.371 367.334 202.371 360.335C202.371 350.779 204.255 343.641 208.021 338.874C211.788 334.107 217.113 331.736 224.018 331.736C237.458 331.736 244.178 341.385 244.178 360.684V364.497H244.154ZM219.159 341.687C217.88 342.641 216.857 344.315 216.113 346.71C215.369 349.105 214.974 352.383 214.927 356.592H232.482C232.459 352.406 232.087 349.105 231.412 346.71C230.738 344.315 229.785 342.641 228.553 341.687C227.32 340.734 225.786 340.246 223.949 340.246C222.042 340.246 220.438 340.734 219.159 341.687Z" fill="#1B1A21"/>
<path d="M260.659 388.655C257.566 388.167 254.776 387.446 252.242 386.54V375.797C258.24 377.75 263.914 378.704 269.285 378.704C273.958 378.704 277.423 377.797 279.725 375.983C282.003 374.17 283.143 371.542 283.143 368.055C283.143 366.101 282.794 364.474 282.096 363.125C281.399 361.777 280.236 360.591 278.609 359.545C276.981 358.498 274.772 357.498 271.935 356.545L267.378 355.011C256.217 351.197 250.66 344.129 250.66 333.828C250.66 329.225 251.66 325.272 253.683 321.97C255.706 318.668 258.705 316.134 262.705 314.367C266.704 312.623 271.656 311.74 277.516 311.74C280.167 311.74 282.817 311.902 285.421 312.251C288.026 312.6 290.327 313.065 292.304 313.693V324.388C288.188 323.086 283.608 322.435 278.585 322.435C268.425 322.435 263.332 325.923 263.332 332.875C263.332 334.758 263.658 336.316 264.286 337.572C264.914 338.827 265.983 339.967 267.448 340.943C268.936 341.943 270.982 342.873 273.61 343.757L278.167 345.291C283.957 347.245 288.351 349.942 291.351 353.406C294.327 356.871 295.838 361.381 295.838 366.939C295.838 371.612 294.815 375.635 292.746 378.983C290.676 382.331 287.677 384.912 283.724 386.702C279.771 388.493 275.005 389.376 269.448 389.376C266.657 389.399 263.728 389.167 260.659 388.655Z" fill="#1B1A21"/>
<path d="M332.25 378.1V388.423C329.553 389.074 326.762 389.4 323.856 389.4C318.02 389.4 313.602 387.865 310.556 384.773C307.51 381.68 305.999 376.774 305.999 370.078V342.501H297.466V332.62H305.999L308.208 312.716H318.392V332.62H331.785V342.501H318.392V368.59C318.392 371.147 318.671 373.17 319.252 374.635C319.834 376.1 320.74 377.146 321.973 377.774C323.205 378.402 324.856 378.727 326.925 378.727C328.181 378.727 329.948 378.518 332.25 378.1Z" fill="#1B1A21"/>
<path d="M341.643 382.47C337.784 377.82 335.854 370.566 335.854 360.753C335.854 341.408 343.317 331.759 358.222 331.759C365.383 331.759 370.894 334.13 374.777 338.874C378.637 343.617 380.59 350.872 380.59 360.66C380.59 379.866 373.126 389.469 358.245 389.469C351.037 389.469 345.503 387.121 341.643 382.47ZM363.523 378.355C364.895 377.262 365.964 375.309 366.732 372.519C367.499 369.728 367.848 365.845 367.848 360.846C367.848 355.731 367.476 351.732 366.732 348.872C365.988 346.012 364.918 344.012 363.5 342.896C362.081 341.78 360.338 341.199 358.222 341.199C356.083 341.199 354.292 341.757 352.897 342.873C351.502 343.989 350.432 345.942 349.688 348.779C348.944 351.616 348.572 355.522 348.572 360.544C348.572 365.636 348.944 369.589 349.688 372.402C350.432 375.239 351.502 377.192 352.897 378.308C354.269 379.424 356.059 379.982 358.245 379.982C360.384 380.006 362.128 379.448 363.523 378.355Z" fill="#1B1A21"/>
<path d="M389.215 332.619H399.446L400.423 340.292H401.027C402.399 337.432 404.236 335.316 406.584 333.968C408.909 332.619 411.56 331.945 414.536 331.945C415.722 331.945 416.908 332.038 418.07 332.247V343.803C416.908 343.594 415.49 343.501 413.769 343.501C411.281 343.501 408.933 344.082 406.7 345.221C404.492 346.361 402.771 347.942 401.585 349.965V388.423H389.215V332.619Z" fill="#1B1A21"/>
<path d="M462.831 364.497H433.627C433.766 368.287 434.324 371.24 435.324 373.403C436.324 375.542 437.835 377.076 439.882 378.006C441.928 378.936 444.648 379.378 448.066 379.378C451.577 379.378 455.646 378.727 460.25 377.425V387.493C457.762 388.167 455.437 388.679 453.274 388.981C451.112 389.283 448.88 389.446 446.601 389.446C440.672 389.446 435.836 388.469 432.092 386.493C428.349 384.517 425.582 381.424 423.768 377.193C421.955 372.937 421.048 367.334 421.048 360.335C421.048 350.779 422.931 343.641 426.698 338.874C430.465 334.107 435.789 331.736 442.695 331.736C456.134 331.736 462.854 341.385 462.854 360.684V364.497H462.831ZM437.835 341.687C436.557 342.641 435.534 344.315 434.789 346.71C434.045 349.105 433.65 352.383 433.604 356.592H451.159C451.135 352.406 450.763 349.105 450.089 346.71C449.415 344.315 448.461 342.641 447.229 341.687C445.997 340.734 444.462 340.246 442.625 340.246C440.719 340.246 439.114 340.734 437.835 341.687Z" fill="#1B1A21"/>
<path d="M237.644 0C265.606 7.77682 291.082 27.3744 301.645 51.327C317.8 90.2111 314.072 139.361 296.363 168.601C281.45 192.554 258.46 205.308 232.052 204.997C191.042 204.686 157.799 172.023 157.488 130.962C157.488 89.9 189.799 55.9931 232.052 55.9931C238.887 55.9931 248.742 56.941 259.927 61.6071C259.927 61.6071 251.886 56.8445 231.235 53.6453C171.895 45.8684 101.876 90.8332 118.031 194.42C140.4 234.238 183.275 261.301 232.052 260.99C303.82 260.679 362.539 201.886 362.228 129.717C362.228 60.6592 305.994 1.86644 237.644 0Z" fill="url(#paint0_linear)"/>
<path d="M286.421 55.0598C276.479 34.218 255.974 17.7311 230.808 11.1986C225.216 9.64319 219.313 9.02103 212.789 8.70996C203.158 8.70996 193.837 9.95429 184.206 12.4429C160.283 19.5975 143.506 33.5958 133.565 44.4833C120.827 59.1037 112.438 74.3463 107.467 89.5889C107.467 89.8999 107.157 90.211 107.157 90.8332C106.846 92.0775 105.603 95.8103 105.603 96.7436C105.292 97.3657 105.292 98.2989 104.982 98.9211C104.671 100.165 104.36 101.41 104.05 102.654C104.05 102.965 104.05 103.276 103.739 103.587C95.6612 143.716 109.08 179.727 117.158 193.103C119.179 196.449 120.838 199.24 122.5 201.801C104.17 102.258 156.866 54.7488 216.828 53.5045C248.207 52.8823 279.585 65.6363 294.809 91.1442C293.877 77.146 292.945 69.3692 286.421 55.0598Z" fill="url(#paint1_linear)"/>
<defs>
<linearGradient id="paint0_linear" x1="317.189" y1="-4.75735e-06" x2="126.668" y2="167.726" gradientUnits="userSpaceOnUse">
<stop stop-color="#FF00FF"/>
<stop offset="0.403975" stop-color="#AA00FF"/>
<stop offset="1" stop-color="#1E0A78"/>
</linearGradient>
<linearGradient id="paint1_linear" x1="198.026" y1="-33.8062" x2="313.821" y2="83.7228" gradientUnits="userSpaceOnUse">
<stop offset="0.0354358" stop-color="#FF00FF"/>
<stop offset="0.512854" stop-color="#8800CC"/>
<stop offset="1" stop-color="#311B92"/>
</linearGradient>
</defs>
</svg>
<span class="text-uppercase font-weight-bold">Cloud-ops</span>
	      </a>

	<div class="td-navbar-nav-scroll ml-md-auto center" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
      <!--
			<li class="nav-item mr-4 mb-2 mb-lg-0">
                            <a class="nav-link" href="https://github.com/vsoch/docsy-jekyll" target="_blank"><span>GitHub</span></a>
			</li>
      -->
      
		</ul>
	</div>

	<div class="navbar-nav d-none d-lg-block">
    <input hidden type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">
  </div>

	<div class="navbar-nav d-none d-lg-block">
    <a class="gh-source" data-gh-source="github" href="https://github.com/vsoch/docsy-jekyll" title="Go to repository" data-md-state="done">
      <div hidden class="gh-source__repository">
        <i class="fab fa fa-github fa-2x" style='padding-right:20px; float:left; margin-top:5px'></i>
            vsoch/docsy-jekyll
        <ul class="gh-source__facts"><li class="gh-source__fact" id='stars'></li><li id="forks" class="gh-source__fact"></li></ul>
      </div>
    </a>
  </div>

</nav>
</header>

<script>
$(document).ready(function() {
  var url = "https://api.github.com/search/repositories?q=vsoch/docsy-jekyll";
  fetch(url, {
      headers: {"Accept":"application/vnd.github.preview"}
  }).then(function(e) {
    return e.json()
  }).then(function(r) {
     console.log(r.items[0])
     stars = r.items[0]['stargazers_count']
     forks = r.items[0]['forks_count']
     $('#stars').text(stars + " Stars")
     $('#forks').text(forks + " Forks")
  });
});
</script>

    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
          <div id="td-sidebar-menu" class="td-sidebar__inner">

  <form class="td-sidebar__search d-flex align-items-center">
    <input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">
      <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type="button" data-toggle="collapse" data-target="#td-section-nav" aria-controls="td-docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
      </button>
  </form>

  <nav class="collapse td-sidebar-nav pt-2 pl-4" id="td-section-nav">
    
      <ul class="td-sidebar-nav__section pr-md-3">
        
        

        <li class="td-sidebar-nav__section-title">
          <a  href="/cloud-ops/docs" class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">Technical Guides</a>
        </li>
        
          <ul>
            <li class="collapse show" id="technical-guides">
              <ul class="td-sidebar-nav__section pr-md-3">
                
                  <li class="td-sidebar-nav__section-title">
                    <a href="
                                /cloud-ops/docs/Deploying/deploying_home
                             "
                       class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">Deploying
                    </a>
                  </li>

                  
                  

                  

                  
                
                  <li class="td-sidebar-nav__section-title">
                    <a href="
                                /cloud-ops/docs/Monitoring/monitoring
                             "
                       class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">Monitoring
                    </a>
                  </li>

                  
                  

                  

                  
                
                  <li class="td-sidebar-nav__section-title">
                    <a href="
                                /cloud-ops/docs/Scaling/scaling
                             "
                       class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">Scaling
                    </a>
                  </li>

                  
                  

                  

                  
                
                  <li class="td-sidebar-nav__section-title">
                    <a href="
                                /cloud-ops/docs/DisasterRecovery/disaster_recovery
                             "
                       class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">DisasterRecovery
                    </a>
                  </li>

                  
                  

                  

                  
                
              </ul>
            </li>
          </ul>
        
      </ul>
    
      <ul class="td-sidebar-nav__section pr-md-3">
        
        

        <li class="td-sidebar-nav__section-title">
          <a  href="" class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">Docker Artifacts</a>
        </li>
        
          <ul>
            <li class="collapse show" id="docker-artifacts">
              <ul class="td-sidebar-nav__section pr-md-3">
                
                  <li class="td-sidebar-nav__section-title">
                    <a href="
                                
                             "
                       class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">memsql/operator
                    </a>
                  </li>

                  
                  

                  

                  
                
                  <li class="td-sidebar-nav__section-title">
                    <a href="
                                
                             "
                       class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">memsql/node
                    </a>
                  </li>

                  
                  

                  

                  
                
                  <li class="td-sidebar-nav__section-title">
                    <a href="
                                
                             "
                       class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">memsql/tools
                    </a>
                  </li>

                  
                  

                  

                  
                
                  <li class="td-sidebar-nav__section-title">
                    <a href="
                                
                             "
                       class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">memsql/cluster-in-a-box
                    </a>
                  </li>

                  
                  

                  

                  
                
              </ul>
            </li>
          </ul>
        
      </ul>
    
  </nav>
</div>

          </div>
          <div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
              <div class="td-page-meta ml-2 pb-1 pt-2 mb-0 box0" hidden>
                  
<!--
<a href="https://github.com/vsoch/docsy-jekyll/edit/master/pages/search.html" target="_blank"><i class="fa fa-edit fa-fw"></i> Edit this page</a>
<a href="https://github.com/vsoch/docsy-jekyll/issues/new?labels=question&title=Question:&body=Question on: https://github.com/vsoch/docsy-jekyll/tree/master/pages/search.html" target="_blank"><i class="fab fa-github fa-fw"></i> Create documentation issue</a>
<a href="https://github.com/vsoch/docsy-jekyll/issues/new" target="_blank"><i class="fas fa-tasks fa-fw"></i> Create project issue</a>
-->

<a href="https://memsql.atlassian.net/secure/CreateIssueDetails!init.jspa?pid=11934&issuetype=10005&summary=Title%20goes%20here&description=Description%20goes%20here&priority=3" target="_blank"><i class="fab fa-jira fa-fw fa-lg"></i> Cloud-ops Jira Support </a>

<!-- this will parse through the header fields and add a button to open
     an issue / ask a question on Github. The editable field should be in
     the post frontend matter, and refer to the label to open the issue for -->

              </div>
              <nav id="TableOfContents"><ul>
              <li><ul id="TOC">
                <!-- Links will be appended here-->
              </ul></li>
              </ul></nav>
          </div>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            <nav aria-label="breadcrumb" class="d-none d-md-block d-print-none">
                <!--
        	      <ol class="breadcrumb spb-1">
                    
                    
                        
                            

                            <li class="breadcrumb-item active" aria-current="page">
                    	          <a href="http://localhost:4000/cloud-ops/search/">Search</a>
                            </li>

                            
                        
                    
        	      </ol>
                -->
                <ol class="breadcrumb spb-1">

                    

                    <!--<a href="/search/">/search/</a>-->

                    
                        
                        
                        

                        
                            <!------------------->
                            

                            <!-- the below variable is created specidifically to check /docs condition. the url is coming as /docs/ -->
                            

                            
                            <!--/search/^^^/docs^^^/docs/^^^Technical Guides-->

                            
                                <!--def-->
                                
                                    
                                        <!--****************-->
                                        
                                        
                                        <!--Deploying^^^/docs/Deploying/deploying_home-->

                                        
                                            <!--%%%%%%%%%%%%%%%%%%%%%-->
                                            
                                                <!--&&&&&&&&&&&&&&&&&&-->
                                                
                                                    
                                                    
                                                    <!--AWS!!!/docs/Deploying/AWS/deploying!!!/search/-->
                                                    
                                                        <!--%%%%%%%%%%%%%%%%%%%%%-->
                                                        <!--<a href="/cloud-ops/docs/Deploying/AWS/deploying">AWS</a>-->
                                                    
                                                
                                                    
                                                    
                                                    <!--GCP!!!/docs/Deploying/GCP/deploying!!!/search/-->
                                                    
                                                        <!--%%%%%%%%%%%%%%%%%%%%%-->
                                                        <!--<a href="/cloud-ops/docs/Deploying/GCP/deploying">GCP</a>-->
                                                    
                                                
                                                    
                                                    
                                                    <!--Azure!!!/docs/Deploying/Azure/deploying!!!/search/-->
                                                    
                                                        <!--%%%%%%%%%%%%%%%%%%%%%-->
                                                        <!--<a href="/cloud-ops/docs/Deploying/Azure/deploying">Azure</a>-->
                                                    
                                                
                                            

                                            <!--<a href=""></a>-->
                                        



                                        

                                    
                                        <!--****************-->
                                        
                                        
                                        <!--Monitoring^^^/docs/Monitoring/monitoring-->

                                        
                                            <!--%%%%%%%%%%%%%%%%%%%%%-->
                                            

                                            <!--<a href=""></a>-->
                                        



                                        

                                    
                                        <!--****************-->
                                        
                                        
                                        <!--Scaling^^^/docs/Scaling/scaling-->

                                        
                                            <!--%%%%%%%%%%%%%%%%%%%%%-->
                                            

                                            <!--<a href=""></a>-->
                                        



                                        

                                    
                                        <!--****************-->
                                        
                                        
                                        <!--DisasterRecovery^^^/docs/DisasterRecovery/disaster_recovery-->

                                        
                                            <!--%%%%%%%%%%%%%%%%%%%%%-->
                                            

                                            <!--<a href=""></a>-->
                                        



                                        

                                    
                                
                            

                            

                        
                            <!------------------->
                            

                            <!-- the below variable is created specidifically to check /docs condition. the url is coming as /docs/ -->
                            

                            
                            <!--/search/^^^/^^^/^^^Docker Artifacts-->

                            
                                <!--def-->
                                
                                    
                                        <!--****************-->
                                        
                                        
                                        <!--memsql/operator^^^/-->

                                        
                                            <!--%%%%%%%%%%%%%%%%%%%%%-->
                                            

                                            <!--<a href=""></a>-->
                                        



                                        

                                    
                                        <!--****************-->
                                        
                                        
                                        <!--memsql/node^^^/-->

                                        
                                            <!--%%%%%%%%%%%%%%%%%%%%%-->
                                            

                                            <!--<a href=""></a>-->
                                        



                                        

                                    
                                        <!--****************-->
                                        
                                        
                                        <!--memsql/tools^^^/-->

                                        
                                            <!--%%%%%%%%%%%%%%%%%%%%%-->
                                            

                                            <!--<a href=""></a>-->
                                        



                                        

                                    
                                        <!--****************-->
                                        
                                        
                                        <!--memsql/cluster-in-a-box^^^/-->

                                        
                                            <!--%%%%%%%%%%%%%%%%%%%%%-->
                                            

                                            <!--<a href=""></a>-->
                                        



                                        

                                    
                                
                            

                            

                        
                        <!--###############-->
                        <!--Docker Artifacts^^^/-->
                        
                        <!--###############-->
                        <!--^^^memsql/cluster-in-a-box^^^/-->
                        

                        <!--###############-->
                        <!--Azure^^^/docs/Deploying/Azure/deploying-->
                        
                    

<!--
                    
                    
                        
                            
                            <li class="breadcrumb-item active" aria-current="page">
                    	          <a href="http://localhost:4000/cloud-ops/search/">Search</a>
                            </li>
                            
                        
                    
                  -->
        	      </ol>
            </nav>


           <div class="td-content">
	      <input class="form-control td-search-input" type="search" name="q" id="search-input" placeholder="&#xf002 Search this site…"  style="margin-top:5px" autofocus>
<i style="color:white; margin-right:8px; margin-left:5px" class="fa fa-search"></i>

<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>

<ul id="search-results"></ul>

<script>
	window.data = {
		
				
					
					
					"docs-deploying-aws-deploying": {
						"id": "docs-deploying-aws-deploying",
						"title": "Deploy SingleStore with Amazon Web Service",
						"categories": "",
						"url": " /docs/Deploying/AWS/deploying",
						"content": "Deploy SingleStore DB in Amazon Elastic Kubernetes Service (EKS)\n\nIntroduction\n\nUse these steps to deploy SingleStore DB in Amazon Elastic Kubernetes\nService (EKS)\n\nSummary\n\nSingleStore’s Cluster Management in kubernetes is different from native\ndeployment. All jobs related to cluster management are done by\noperators.\n\nReference:\nhttps://docs.singlestore.com/db/v7.3/en/reference/memsql-operator-reference/memsql-operator-reference-overview.html\n\nUser prohibited to use below command or tools:\n\n\n  \n    Cluster management command\n  \n  \n    SingleStore DB Toolbox except sdb-report\n\n    a.  sdb-toolbox-config. Performs host machine registration.\n\n    b.  sdb-deploy. Installs memsqlctl and the SingleStore DB database engine to host machines in the cluster.\n\n    c.  sdb-admin. Helps you manage a SingleStore DB cluster.\n\n    d.  sdb-report. Collects and performs diagnostic checks on your cluster.\n\n    e.  memsqlctl. Provides lower-level access to manage nodes on a host machine.\n  \n\n\nEKS Cluster\n\nPrerequisite\n\nRole\n\nCreate two roles\n\nCluster IAM Role\n\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html\n\n\n  \n    Open the IAM console at https://console.aws.amazon.com/iam/\n  \n  \n    In the left navigation pane, choose Roles.\n  \n  \n    Search the list of roles for eksClusterRole.\n If a role that includes eksClusterRole doesn't exist, then continue the next step to create the role else skip the rest of steps.\n  \n  \n    Click Create role\n\n    \n  \n  \n    Select trusted entity\n  \n  \n    Select AWS service\n  \n  \n    Select EKS - Cluster as Use cases\n\n    \n  \n  \n    Add permissions\n\n    \n  \n  \n    Name, review, and create\n\n    \n  \n\n\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/security-iam-awsmanpol.html\n\nAssign all *EKS* managed policy to the role\n\n\n\nService IAM Role\n\n\n  \n    Open the IAM console at https://console.aws.amazon.com/iam/\n  \n  \n    In the left navigation pane, choose Roles.\n  \n  \n    Search the list of roles for AWSServiceRoleForAmazonEKSNodegroup.\n If a role that includes AWSServiceRoleForAmazonEKSNodegroup doesn't exist, then continue the next step to create the role else skip the rest of steps.\n  \n  \n    Click Create role\n\n    \n  \n  \n    Select trusted entity\n  \n  \n    Select AWS service\n  \n  \n    Select EKS - Nodegroup as Use cases\n\n    \n  \n  \n    Add permissions\n\n    \n  \n  \n    Name, review, and create\n\n    \n  \n\n\nNode IAM Role\n\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html\n\n\n  \n    Open the IAM console at https://console.aws.amazon.com/iam/\n  \n  \n    In the left navigation pane, choose Roles.\n  \n  \n    Search the list of roles for AmazonEKSNodeRole.\n If a role that includes AWSServiceRoleForAmazonEKS doesn't exist, then continue the next step to create the role else skip the rest of steps.\n  \n  \n    Click Create role\n\n    \n  \n  \n    Select trusted entity\n  \n  \n    Select AWS service\n  \n  \n    Select EC2 as Use cases\n\n    \n  \n  \n    Add permissions, select AmazonEKSWorkerNodePolicy and AmazonEC2ContainerRegistryReadOnly\n\n    \n\n    \n  \n  \n    Name, review, and create\n\n    \n  \n\n\nPolicy\n\n\n  \n    Open the IAM console at https://console.aws.amazon.com/iam/\n  \n  \n    In the left navigation pane, choose Policies.\n  \n  \n    Click Create Policy.\n\n    \n  \n  \n    Copy the following aws permission on JSON tab\n\n    \n  \n\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Sid\": \"VisualEditor0\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"iam:ListRoles\",\n\n\"ec2:DescribeSubnets\",\n\n\"eks:CreateCluster\"\n\n],\n\n\"Resource\": \"*\"\n\n},\n\n{\n\n\"Sid\": \"VisualEditor1\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"eks:DeleteCluster\",\n\n\"iam:GetRole\",\n\n\"iam:PassRole\",\n\n\"iam:ListAttachedRolePolicies\",\n\n\"eks:DeleteNodegroup\",\n\n\"eks:TagResource\",\n\n\"eks:DescribeCluster\",\n\n\"eks:CreateNodegroup\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:iam::[account-id]:role/AmazonEKSNodeRole\",\n\n\"arn:aws:iam::[account-id]:role/eksClusterRole\",\n\n\"arn:aws:iam::[account-id]:role/*AWSServiceRoleForAmazonEKSNodegroup\",\n\n\"arn:aws:eks:us-east-1:[account-id]:nodegroup/eks-cluster/*/*\",\n\n\"arn:aws:eks:us-east-1:[account-id]:cluster/eks-cluster\"\n\n]\n\n}\n\n]\n\n}\n\n\n  \n    Review policy, put name eks-policy\n\n    \n  \n\n\nUser\n\nCreate user service\n\n\n  \n    Open the IAM console at https://console.aws.amazon.com/iam/\n  \n  \n    In the left navigation pane, choose Users.\n  \n  \n    Set user details\n  \n  \n    Select Access key - Programmatic access only\n\n    \n  \n  \n    Add permission\n  \n  \n    Choose Attach existing policies directly.\n Attach eks-policy to users.\n\n    \n  \n\n\nCreate the Cluster\n\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html#aws-cli\n\n\n  Define\n\n\nregion-code\n\ncluster-name\n\naccount_id\n\nsubnetId\n\n\n  Create cluster\n\n\nUse comma as delimiter if you have more than one subnet\n\nCreate your cluster with the following command:\n\naws eks create-cluster \\\n\n--region region-code \\\n\n--name cluster-name \\\n\n--kubernetes-version 1.21 \\\n\n--role-arn arn:aws:iam::[account-id]:role/eksClusterRole \\\n\n--resources-vpc-config subnetIds=subnetId1,subnetId2\n\n![](../../assets/img/docs_deploy_aws_media/media/image44.png)\n\n\nThis process may take several minutes.\n\nAccess the Cluster\n\nConnect kubectl\n\naws eks update-kubeconfig --region us-east-1 --name eks-cluster\n\nVerify\n\nkubectl get node -owide\n\n{width=”6.267716535433071in” height=”0.4722222222222222in”}\n\nDelete VPC CNI\n\nReference:\nhttps://docs.cilium.io/en/v1.9/gettingstarted/k8s-install-eks/\n\nCilium will manage ENIs instead of VPC CNI, so the aws-node DaemonSet\nhas to be deleted to prevent conflict behavior.\n\nkubectl -n kube-system delete daemonset aws-node\n\n\n\nWait the cluster creation complete if you above result\n\n\n\nYou can continue to the next step using kubectl command\n\nDeploy Cilium as CNI\n\nReference:\nhttps://docs.cilium.io/en/v1.9/gettingstarted/k8s-install-eks/#deploy-cilium\n\nSetup Helm repository:\n\nhelm repo add cilium https://helm.cilium.io/\n\nThis helm command sets eni=true and tunnel=disabled, meaning the Cilium\nwill allocate a fully-routable AWS ENI IP address for each pod, similar\nto the behavior of the Amazon VPC CNI\nplugin.\n\nExcluding the lines for eni=true, ipam.mode=eni and tunnel=disabled from\nthe helm command will configure Cilium to use overlay routing mode.\nEnsure the pod CIDR (ipam.operator.clusterPoolIPv4PodCIDR) is not\noverlapping with node CIDR.\n\nDeploy Cilium release via Helm:\n\nhelm install cilium cilium/cilium --version 1.9.13 \\\n\n--namespace kube-system \\\n\n--set eni=true \\\n\n--set ipam.mode=eni \\\n\n--set egressMasqueradeInterfaces=eth0 \\\n\n--set tunnel=disabled \\\n\n--set nodeinit.enabled=true\n\nCreate the Node Group\n\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html#aws-cli\n\n\n  Define\n\n\nregion-code\n\ncluster-name\n\naccount_id\n\nsubnetId\n\n\n  Create node group\n\n\nUse space as delimiter if you have more than one subnet\n\nCreate node group with the following command:\n\naws eks create-nodegroup \\\n\n--cluster-name cluster-name \\\n\n--region region-code \\\n\n--nodegroup-name ng-1 \\\n\n--scaling-config minSize=1,maxSize=2,desiredSize=1 \\\n\n--instance-types t2.large \\\n\n--subnets subnetId1 subnetId2 \\\n\n--node-role arn:aws:iam::[accountId]:role/AmazonEKSNodeRole\n\n![](../../assets/img/docs_deploy_aws_media/media/image48.png)\n\n\nThis process may take several minutes.\n\nVerification\n\n\n  Ensure the node’s status is Ready and all pod’s status Running.\n\n\nkubectl get nodes\n\nkubectl get po -A\n\n![](../../assets/img/docs_deploy_aws_media/media/image49.png)\n\n\n\n  In cilium operator logs show Initialization complete\n\n\nkubectl logs [cilium-operator-pod-name] -nkube-system --tail 10\n\n![](../../assets/img/docs_deploy_aws_media/media/image51.png)\n\n\nSingleStore\n\nCluster Admin Prerequisites\n\n\n  \n    Determine the project (namespace) in which to deploy SingleStore DB.\n  \n  Determine which StorageClass (SC) to use.\n Avoid using a StorageClass with an NFS-based provisioner. Ideally, you should choose a StorageClass that uses a block storage-based provisioner that supports volume expansion and the WaitForFirstConsumer binding mode.\n    \n      Available SC in EKS:\n    \n\n    \n  \n  Determine the fsGroup to use for the deployment.\n\n\nDeployment Prerequisites\n\n\n  \n    Obtain a SingleStore license from the SingleStore Customer Portal.\n  \n  \n    Select the SingleStore DB images to use. Two Docker images are required for the deployment.\n\n    a.  The node image is the SingleStore DB database engine and can be found on Docker Hub.\n\n    b.  The Operator image is used to manage the SingleStore DB engine deployment in Kubernetes environment, and can also be found on Docker Hub.\n  \n  \n    Use the StorageClass that you selected in Cluster Admin Prerequisites.\n  \n  \n    Substitute the fsGroup value with the value you copied in Cluster Admin Prerequisites.\n  \n\n\nCreate the Object Definition Files\n\nThe following are definition files that will be used by the Operator to\ncreate your cluster. Create new definition files and copy and paste the\ncontents of each code block into those files.\n\n\n  \n    deployment.yaml\n  \n  \n    rbac.yaml\n  \n  \n    memsql-cluster-crd.yaml\n  \n  \n    memsql-cluster.yaml\n  \n\n\n###\n\nMemSQL Operator\n\nFor --cluster-id, enter the name of your cluster. This will be the name\nused in the memsql-cluster.yaml file as per:\n\nmetadata:\n\nname: memsql-cluster\n\nCreate a deployment definition file using the template below.\n\ncat &gt; deployment.yaml &lt;&lt;EOF\n\napiVersion: apps/v1\n\nkind: Deployment\n\nmetadata:\n\nname: memsql-operator\n\nspec:\n\nreplicas: 1\n\nselector:\n\nmatchLabels:\n\nname: memsql-operator\n\ntemplate:\n\nmetadata:\n\nlabels:\n\nname: memsql-operator\n\nspec:\n\nserviceAccountName: memsql-operator\n\ncontainers:\n\n- name: memsql-operator\n\nimage: memsql/operator:1.2.5-83e8133a\n\nimagePullPolicy: Always\n\nargs:\n\n- \"--cores-per-unit\"\n\n- \"8\"\n\n- \"--memory-per-unit\"\n\n- \"32\"\n\n- \"--merge-service-annotations\"\n\n- \"--cluster-id\"\n\n- \"memsql-cluster\"\n\n- \"--fs-group-id\"\n\n- \"5555\"\n\nenv:\n\n- name: WATCH_NAMESPACE\n\nvalueFrom:\n\nfieldRef:\n\nfieldPath: metadata.namespace\n\n- name: POD_NAME\n\nvalueFrom:\n\nfieldRef:\n\nfieldPath: metadata.name\n\n- name: OPERATOR_NAME\n\nvalue: memsql-operator\n\n- name: RELATED_IMAGE_BACKUP\n\nvalue: memsql/tools\n\nEOF\n\nRBAC\n\nCopy the following to create a ServiceAccount definition file that will\nbe used by the MemSQL Operator.\n\ncat &gt; rbac.yaml &lt;&lt;EOF\n\napiVersion: v1\n\nkind: ServiceAccount\n\nmetadata:\n\nname: memsql-operator\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\n\nkind: Role\n\nmetadata:\n\nname: memsql-operator\n\nrules:\n\n- apiGroups:\n\n- \"\"\n\nresources:\n\n- pods\n\n- services\n\n- endpoints\n\n- persistentvolumeclaims\n\n- events\n\n- configmaps\n\n- secrets\n\nverbs:\n\n- '*'\n\n- apiGroups:\n\n- policy\n\nresources:\n\n- poddisruptionbudgets\n\nverbs:\n\n- '*'\n\n- apiGroups:\n\n- batch\n\nresources:\n\n- cronjobs\n\nverbs:\n\n- '*'\n\n- apiGroups:\n\n- \"\"\n\nresources:\n\n- namespaces\n\nverbs:\n\n- get\n\n- apiGroups:\n\n- apps\n\n- extensions\n\nresources:\n\n- deployments\n\n- daemonsets\n\n- replicasets\n\n- statefulsets\n\n- statefulsets/status\n\nverbs:\n\n- '*'\n\n- apiGroups:\n\n- memsql.com\n\nresources:\n\n- '*'\n\nverbs:\n\n- '*'\n\n- apiGroups:\n\n- networking.k8s.io\n\nresources:\n\n- networkpolicies\n\nverbs:\n\n- '*'\n\n- apiGroups:\n\n- coordination.k8s.io\n\nresources:\n\n- leases\n\nverbs:\n\n- get\n\n- list\n\n- watch\n\n- create\n\n- update\n\n- patch\n\n- delete\n\n---\n\nkind: RoleBinding\n\napiVersion: rbac.authorization.k8s.io/v1\n\nmetadata:\n\nname: memsql-operator\n\nsubjects:\n\n- kind: ServiceAccount\n\nname: memsql-operator\n\nroleRef:\n\nkind: Role\n\nname: memsql-operator\n\napiGroup: rbac.authorization.k8s.io\n\nEOF\n\nCustom Resource Definition\n\n\n  \n    Update apiVersion from v1alpha to v1\n  \n  \n    Restructure the spec\n  \n  \n    Create the CRD file use following command\n  \n\n\ncat &gt; memsql-cluster-crd.yaml &lt;&lt;EOF\n\napiVersion: apiextensions.k8s.io/v1\n\nkind: CustomResourceDefinition\n\nmetadata:\n\nname: memsqlclusters.memsql.com\n\nspec:\n\ngroup: memsql.com\n\nnames:\n\nkind: MemsqlCluster\n\nlistKind: MemsqlClusterList\n\nplural: memsqlclusters\n\nsingular: memsqlcluster\n\nshortNames:\n\n- memsql\n\nscope: Namespaced\n\nversions:\n\n- name: v1alpha1\n\nserved: true\n\nstorage: true\n\nschema:\n\nopenAPIV3Schema:\n\ntype: object\n\nx-kubernetes-preserve-unknown-fields: true\n\nadditionalPrinterColumns:\n\n- name: Aggregators\n\ntype: integer\n\ndescription: Number of SingleStore DB Aggregators\n\njsonPath: .spec.aggregatorSpec.count\n\n- name: Leaves\n\ntype: integer\n\ndescription: Number of SingleStore DB Leaves (per availability group)\n\njsonPath: .spec.leafSpec.count\n\n- name: Redundancy Level\n\ntype: integer\n\ndescription: Redundancy level of SingleStore DB Cluster\n\njsonPath: .spec.redundancyLevel\n\n- name: Age\n\ntype: date\n\njsonPath: .metadata.creationTimestamp\n\nEOF\n\nMemSQL Cluster\n\nCreate a MemSQLCluster definition file to specify the configuration\nsettings for your cluster.\n\ncat &gt; memsql-cluster.yaml &lt;&lt;EOF\n\napiVersion: memsql.com/v1alpha1\n\nkind: MemsqlCluster\n\nmetadata:\n\nname: memsql-cluster\n\nspec:\n\n# TODO: paste your license key from https://portal.singlestore.com\nhere:\n\nlicense: REPLACE_THIS_WITH_LICENSE\n\n# TODO: replace the default admin password for production environment\n\n# select password(\"secret\");\n\nadminHashedPassword: \"*14E65567ABDB5135D0CFD9A70B3032C179A49EE7\"\n\nnodeImage:\n\nrepository: memsql/node\n\ntag: latest\n\n# TODO: set greater than 1 to enable HA mode\n\nredundancyLevel: 1\n\nserviceSpec:\n\nobjectMetaOverrides:\n\nlabels:\n\ncustom: label\n\nannotations:\n\ncustom: annotations\n\naggregatorSpec:\n\ncount: 1\n\nheight: 0.25\n\nstorageGB: 20\n\nstorageClass: default\n\nobjectMetaOverrides:\n\nannotations:\n\noptional: annotation\n\nlabels:\n\noptional: label\n\nleafSpec:\n\ncount: 1\n\nheight: 0.25\n\nstorageGB: 20\n\nstorageClass: default\n\nobjectMetaOverrides:\n\nannotations:\n\noptional: annotation\n\nlabels:\n\noptional: label\n\nEOF\n\nDeploy a SingleStore DB Cluster\n\nReference: Deploy a SingleStore DB\nCluster\n\nNow that your various object definition files are created, you will use\nkubectl to do the actual object creation and cluster deployment.\n\n\n  Install the RBAC resources.\n\n\nkubectl create -f rbac.yaml -n&lt;namespace&gt;\n\n\n  Install the MemSQL cluster resource definition.\n\n\nkubectl create -f memsql-cluster-crd.yaml\n\n\n  Deploy the Operator.\n\n\nkubectl create -f deployment.yaml -n&lt;namespace&gt;\n\n\n  Verify the deployment was successful by checking the status of the pods in your Kube cluster. You should see the Operator with a status of Running.\n\n\nkubectl get pods\n\n\n  Finally, create the cluster.\n\n\nkubectl create -f memsql-cluster.yaml\n\n\n  After a couple minutes, run kubectl get pods again to verify the aggregator and leaf nodes all started and have a status of Running.\n\n\nkubectl get pods\nIf you see no pods are in the Running state, check the Operator logs by\nrunning\n\nkubectl logs deployment memsql-operator -n&lt;namespace&gt;\n\nthen look at the various objects to see what is failing.\n\nVerification\n\n\n  Verify the cluster\n\n\nkubectl get memsqlcluster memsql-cluster \\\n\n-o=jsonpath='{.status.phase}{\"\\n\"}' -n&lt;name-space&gt;\n\nThe SingleStore DB server deployment is complete when Running is\ndisplayed after running the above commands.\n\n\n  Verify the pod list, run the following command to display the pod list.\n\n\nkubectl get po -n&lt;name-space&gt;\n\nResult may vary:\n\n\n\n\n  After the deployment completes, run the following command to display the two SingleStore DB service endpoints that are created during the deployment.\n\n\nkubectl get svc | grep &lt;cluster-name&gt;\n\nThe svc-&lt;cluster-name&gt;-ddl and svc-&lt;cluster-name&gt;-dml service\nendpoints can be used to connect to SingleStore DB using a MySQL\ncompatible client. Note that svc-&lt;cluster-name&gt;-dml only exists if\nmemsqlCluster.aggregatorSpec.count is greater than 1.\n\nThe output will resemble the following (actual values will vary):\n\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\n\nsvc-memsql-cluster ClusterIP None &lt;none&gt; 3306/TCP 44h\n\nsvc-memsql-cluster-ddl LoadBalancer 10.101.189.86 169.46.26.10\n3306:30351/TCP 44h\n\nsvc-memsql-cluster-dml LoadBalancer 10.103.29.220 169.46.26.11\n3306:32524/TCP 44h\n\nsvc-memsql-studio LoadBalancer 10.97.104.121 169.46.26.11 8081:31161/TCP\n43h\n\nColumn definition:\n\nNAME EXTERNAL-IP PORT(S) AGE\n\nService name External IP Service Port:Node Port Service created\n\nRefer to Data Definition Language\nDDL\nand Data Manipulation Language\nDML\nfor more information.\n\n#\n\nInstall SingleStore Client\n\n\n  Add the SingleStore repository to your repository list.\n\n\nsudo yum-config-manager --add-repo\nhttps://release.memsql.com/production/rpm/x86_64/repodata/memsql.repo\n\n\n  Verify that the SingleStore repo information is listed under repolist.\n\n\nsudo yum repolist\n\n\n  Verify that the which package is installed. This is used during the install process to identify the correct package type for your installation.\n\n\nrpm -q which\n\n\n  Install which\n    \n      Skip this step if you have it\n    \n  \n\n\nsudo yum install -y which\n\n\n  Install the SingleStore client.\n\n\nsudo yum install -y singlestore-client\n\nAccess SingleStore DB\n\n\n  Connect via Load Balancer / External IP (refer svc-memsql-cluster-ddl endpoint)\n\n\nsinglestore -u admin -h &lt;external-ip&gt; -p\n\n![](../../assets/img/docs_deploy_aws_media/media/image56.png)\n\n\n\n  Connect via Node Port (refer svc-memsql-cluster-ddl endpoint)\n In case the External-IP is pending, we can access through Node Port Get the host IP of singlestore’s master node:\n\n\nkubectl get po node-memsql-cluster-master-0 -n&lt;namespace&gt;\n-o=jsonpath='{.status.hostIP}{\"\\n\"}'\n\n\n  Sample output: 172.31.21.171\n\n\nsinglestore -u admin -h &lt;host-ip&gt; -P &lt;node-port&gt; -p\n\n![](../../assets/img/docs_deploy_aws_media/media/image17.png)\n\n\n\n  Check status of aggregator and leaf, run the following command\n\n\nshow aggregators;\n\nshow leaves;\n\n\n  The result may vary:\n\n\n![](../../assets/img/docs_deploy_aws_media/media/image11.png)\n\n\nScaling\n\nWe need to modify the resource only. All jobs are done by operators. You\nneed to monitor the log of the pod operator during scaling to identify\nif the scaling is in progress, done or has a problem.\n\nPrerequisites\n\n\n  \n    Ensure the backup operator is running and the backup file exists.\n  \n  \n    Get the memsql cluster\n  \n\n\nkubectl get memsqlcluster -n&lt;namespace&gt;\n\n![](../../assets/img/docs_deploy_aws_media/media/image20.png)\n\n\n\n  Check current spec of MemSQL cluster\n\n\nkubectl get memsqlcluster memsql-cluster -n&lt;namespace&gt; -oyaml\n\n![](../../assets/img/docs_deploy_aws_media/media/image12.png)\n\n\n\n  Definition\n\n\ncount: number of nodes\n\nheight: cpu &amp; memory limit\n\nstorageGB: storage size\n\n\n  Monitor log of operator\n\n\nOpen a new terminal window.\n\nkubectl get po -n&lt;namespace&gt;\n\n![](../../assets/img/docs_deploy_aws_media/media/image13.png)\n\n\nkubectl logs memsql-operator-&lt;x&gt; -n&lt;namespace&gt; -f\n\n![](../../assets/img/docs_deploy_aws_media/media/image18.png)\n\n\n\n  Monitor the pod during scaling\n\n\nOpen a new terminal window\n\nwatch kubectl get po -n&lt;namespace&gt;\n\n![](../../assets/img/docs_deploy_aws_media/media/image14.png)\n\n\nHorizontal Scaling\n\nIncreasing the number of cluster nodes. Adding an aggregator or leaf\nnodes.\n\n\n  Create scaling patch file:\n\n\ncat &gt; scaling.yaml &lt;&lt;EOF\n\nspec:\n\naggregatorSpec:\n\ncount: 1\n\nleafSpec:\n\ncount: 2\n\nEOF\n\n\n  Apply your changes on memsql cluster by patching the object using patching file\n\n\nkubectl patch memsqlcluster memsql-cluster --type merge --patch-file\nscaling.yaml -n&lt;namespace&gt;\n\n![](../../assets/img/docs_deploy_aws_media/media/image15.png)\n\n\n\n  Verification\n\n\nNew leaves pod created\n\nkubectl get po -n&lt;namespace&gt;\n\n![](../../assets/img/docs_deploy_aws_media/media/image16.png)\n\n\nVertical Scaling\n\nIncreasing node’s resources, such as cpu, memory or storage.\n\nCPU &amp; Memory\n\nRequirement:\n\nThe number of cpu &amp; memory defined in args of operator should be greater\nthan zero\n\nkubectl get deployment memsql-operator -n&lt;namespace&gt; -oyaml\n\n![](../../assets/img/docs_deploy_aws_media/media/image21.png)\n\n\nNote: You can do vertical scaling in the current cluster if and only\nif the number of cores-per-unit or memory-per-unit is greater than zero\nelse you need to recreate a new cluster then reattach the storage.\n\nWe have two ways to do vertical scaling\n\nOperator\n\nModify the operator configuration\n\n\n  Edit the memsql-operator deployment\n\n\nkubectl edit deploy memsql-operator -n&lt;namespace&gt;\n\n![](../../assets/img/docs_deploy_aws_media/media/image3.png)\n\n\n\n  \n    Change the number of cores-per-unit or memory-per-unit. Ensure the number greater than zero.\n  \n  \n    Save the changes\n  \n\n\nResult:\n\n\n  \n    Old pod operator will destroy\n  \n  \n    New pod operator will created\n  \n  \n    Monitor log of new operator\n  \n\n\nkubectl logs memsql-operator-&lt;x&gt; -n&lt;namespace&gt; -f\n\n\n  Let’s\n\n\nCluster\n\nModify the node’s spec\n\n\n  Create scaling patch file\n\n\ncat &gt; scaling.yaml &lt;&lt;EOF\n\nspec:\n\naggregatorSpec:\n\nheight: 0.125\n\nleafSpec:\n\nheight: 0.25\n\nEOF\n\n\n  Get the pods\n\n\nkubectl get po -n&lt;namespace&gt;|grep node\n\nsample output:\n\n![](../../assets/img/docs_deploy_aws_media/media/image1.png)\n\n\n\n  Check the resource before changes\n\n\nkubectl get pods &lt;pod-name&gt; -n&lt;namespace&gt; -o jsonpath=\\\n\n'{range .spec.containers[?(@.name==\"node\")]}{\"Container Name:\n\"}{.name}{\"\\n\"}{\"Requests:\"}{.resources.requests}{\"\\n\"}{\"Limits:\"}{.resources.limits}{\"\\n\"}{end}'\n\nsample output:\n\n![](../../assets/img/docs_deploy_aws_media/media/image2.png)\n\n\n\n  Apply your changes on memsql cluster by patching the object using patching file\n\n\nkubectl patch memsqlcluster memsql-cluster --type merge --patch-file\nscaling.yaml -n&lt;namespace&gt;\n\n\n  Verification\n\n\nVerify the number of cpu &amp; memory is increase as per desired\n\nkubectl get pod &lt;pod-name&gt; -n&lt;namespace&gt; -o jsonpath=\\\n\n'{range .spec.containers[?(@.name==\"node\")]}{\"Container Name:\n\"}{.name}{\"\\n\"}{\"Requests:\"}{.resources.requests}{\"\\n\"}{\"Limits:\"}{.resources.limits}{\"\\n\"}{end}'\n\n![](../../assets/img/docs_deploy_aws_media/media/image4.png)\n\n\nStorage\n\n\n  Requirement\n\n\n&lt;!-- --&gt;\n\na.  The storage class should be allow volume expansion\n\nkubectl get sc &lt;sc-name&gt; -oyaml\n\n![](../../assets/img/docs_deploy_aws_media/media/image7.png)\n\n\nb.  Support modifying a Disk Size to a larger size only\n\n&lt;!-- --&gt;\n\n\n  Check storage\n\n\nCheck the capacity of pvc/pv.\n\nkubectl get pvc -n&lt;namespace&gt;\n\nkubectl get pv\n\n![](../../assets/img/docs_deploy_aws_media/media/image5.png)\n\n\n\n  Create scaling patch file\n\n\ncat &gt; scaling.yaml &lt;&lt;EOF\n\nspec:\n\naggregatorSpec:\n\nstorageGB: 20\n\nleafSpec:\n\nstorageGB: 25\n\nEOF\n\n\n  Apply your changes on memsql cluster by patching the object using patching file\n\n\nkubectl patch memsqlcluster memsql-cluster --type merge --patch-file\nscaling.yaml -n&lt;namespace&gt;\n\n\n  Verify\n\n\nCheck the capacity of pvc/pv.\n\nkubectl get pvc -n&lt;namespace&gt;\n\nkubectl get pv\n\n![](../../assets/img/docs_deploy_aws_media/media/image6.png)\n\n\nTroubleshoot\n\n\n  Error during resize the storage\n\n\nkubectl describe pvc &lt;pvc-name&gt; -n&lt;namespace&gt;\n\n![](../../assets/img/docs_deploy_aws_media/media/image8.png)\n\n\nError message in pod operator:\n\nerrors.go:92 {controller.memsql} Reconciler error will retry after:\n\"55s\" error: \"Waiting for PVC\npv-storage-node-memsql-cluster-leaf-ag1-0 to finish controller\nresizing\"\n\nRoot cause:\n\nThe storage provisioner required detach the pod before resizing the\nvolume\n\nSolution:\n\n\n  Delete the pod\n\n\nkubectl delete po &lt;pod-name&gt; -n&lt;namespace&gt;\n\n\n  Downsizing the replicas on related statefulset if option 1 doesn’t effect:\n\n\nkubectl -n&lt;namespace&gt; patch sts &lt;statefulset-name&gt; --type merge -p\n'{\"spec\":{\"replicas\": 0}}'\n\nResult:\n\n\n  \n    The pod will be deleted\n  \n  \n    PVC will be resized\n  \n  \n    The new pod will recreate\n  \n\n\nMonitoring\n\nReference:\nhttps://docs.singlestore.com/db/v7.6/en/user-and-cluster-administration/cluster-health-and-performance/monitoring/configure-monitoring.html\n\nIntroduction\n\nSingleStore’s native monitoring solution is designed to capture and\nreveal SingleStore DB cluster events over time. By analyzing this event\ndata, you can identify trends and, if necessary, take action to\nremediate issues. Use these steps to Configure SingleStore DB Monitoring\nin Kubernetes Environment.\n\nTerminology\n\nThroughout this guide, the cluster that is being monitored will be\nreferred to as the “Source” cluster, and the cluster that stores the\nmonitoring data will be referred to as the “Metrics” cluster. The\ndatabases that store monitoring data will be referred to as the metrics\ndatabase.\n\nHigh-Level Architecture\n\n\n\nIn SingleStore’s native monitoring solution, the Metrics cluster\nutilizes a SingleStore pipeline to pull the data from the exporter\nprocess on the Source cluster and stores it in a database named metrics.\nNote that this metrics database can either reside within the same\ncluster as the Source cluster, or within a dedicated cluster.\n\nWhen these event data is then analyzed through the associated Grafana\ndashboards, trends can be identified and, if necessary, actions taken to\nremediate issues.\n\nThe provided Grafana dashboards include:\n\n\nDashboard       Description\n  ——————- —————————————————\n  Active session      Aggregated resource consumption by activity and\n  history             activity type\n\nActivity history    Resource consumption by a specific activity over\n                      time\n\nDetailed cluster    A “birds-eye view” of a single SingleStore DB\n  view                cluster\n\nInformation schema  Provides a view into information_schema views\n  view                PROCESSLIST, TABLES, and TABLE_STATISTICS\n\nMemory usage        Granular breakdown of memory use for a host\n\nSingleStore DB      Collected status variables from each host in the\n  status and          cluster\n  variables view\n\nNode breakout       System metrics from each host in the cluster\n\nNode drilldown      System metrics from each host in the cluster, with\n                      the ability to focus on a specific metric subsystem\n  ———————————————————————–\n\nPrerequisites\n\n\n  \n    A SingleStore DB 7.3 or later cluster to monitor (the Source cluster).\n  \n  \n    Optional: A separate SingleStore DB 7.3 or later cluster to collect monitoring data (the Metrics cluster).\n\n    a.  This can be the same as, or separate from, the Source cluster.\n\n    b.  If you opt to use a separate cluster, we recommend a cluster with two aggregator nodes and two leaf nodes, each with 2TB disks and with high availability (HA) enabled.\n  \n  \n    A Grafana 6.0.0 or later instance that can access the Metrics cluster.\n  \n\n\n##\n\nPort Configuration\n\n\nDefault Port  Used by              Invoked by\n  —————– ———————— —————————-\n  3000              Grafana                  User browser\n\n3306              SingleStore DB           memsql_exporter\n\n8080              SingleStore DB Studio    User browser\n\n9104              memsql_exporter          SingleStore pipelines\n  ———————————————————————–\n\nIdentify Exporter Process\n\n\n  Check the exporter’s port\n\n\n\n  kubectl exec -it &lt;master-pod&gt; -n&lt;namespace&gt; -- curl -v telnet://node-memsql-cluster-master-0:9104\n\n\nSample output:\n\n![](../../assets/img/docs_deploy_aws_media/media/image9.png)\n\n\n\n  Test the metrics\n\n\ncurl http://node-memsql-cluster-master-0:9104\n\nSample output:\n\n![](../../assets/img/docs_deploy_aws_media/media/image36.png)\n\n\nConfigure the metrics Database\n\nCreate the metrics database and associated tables\n\nDownload the sql file below, extract and run in metric DB.\n\nmetrics-database-ddl_73.sql\n\nCreate monitoring user\n\n\n  Access the metric cluster using mysql client as admin\n\n\nmysql -u admin -h &lt;hostname or ip&gt; -P &lt;port&gt; -p\n Enter the admin password respectively.\n\nSample output:\n\n![](../../assets/img/docs_deploy_aws_media/media/image30.png)\n\n\n\n  Create the monitoring user\n\n\nCREATE USER 'dbmon'@'%' IDENTIFIED BY &lt;password&gt;';\n\n\n  Grant SELECT privilege to monitoring user\n GRANT SELECT, EXECUTE ON metrics.* TO 'dbmon'@'%';\n\n\nCreate the pipeline\n\nNote: You must edit exporter-host and port in the following SQL\nstatements to align with where your exporter process resides.\n\n\n  \n    The exporter-host is typically the host of your Source cluster’s Master Aggregator that’s running the exporter and must include http://.\n  \n  \n    The default port for the endpoint is 9104.\n  \n\n\nThe metrics pipeline\n\n\n  \n    Get the master host\n show aggregators;\n\n    \n  \n  \n    Create the pipeline\n  \n\n\n\n  CREATE OR REPLACE PIPELINE `metrics` AS\n\n  LOAD DATA prometheus_exporter\n\n  \"&lt;exporter-host:port&gt;/cluster-metrics\"\n\n  CONFIG '{\"is_memsql_internal\":true}'\n\n  INTO PROCEDURE `load_metrics` FORMAT JSON;\n\n  Sample output:\n\n\n![](../../assets/img/docs_deploy_aws_media/media/image34.png)\n\n\n\n  Test and verify the pipeline\n\n\n\n  TEST PIPELINE metrics;\n\n  You should see some data after running that command.\n\n\n![](../../assets/img/docs_deploy_aws_media/media/image35.png)\n\n\n\n  Run the pipeline\n\n\n\n  START PIPELINE IF NOT RUNNING metrics;\n\n\nThe blobs pipeline\n\n\n  Create the pipeline\n\n\n\n  CREATE OR REPLACE PIPELINE `blobs` AS\n\n  LOAD DATA prometheus_exporter\n\n  \"&lt;exporter-host:port&gt;/samples\"\n\n  CONFIG '{\"is_memsql_internal\":true, \"download_type\":\"samples\",\n\"monitoring_version\": \"7.3\"}'\n\n  INTO PROCEDURE `load_blobs` FORMAT JSON;\n\n  Sample output:\n\n\n![](../../assets/img/docs_deploy_aws_media/media/image37.png)\n\n\n\n  Test and verify the pipeline\n\n\n\n  TEST PIPELINE blobs;\n\n\n\n  Run the pipeline\n\n\n\n  START PIPELINE IF NOT RUNNING blobs;\n\n\nTroubleshoot the pipeline\n\nIf you see some error like below screenshot, please double check the\nexporter-host\n\n\n\nDrop the existing pipeline and recreate the pipeline\n\nDROP PIPELINE metrics;\n\nSample output:\n\n\n\nGrafana\n\nSetup\n\nWe will install grafana using helm chart\n\n\n  Add grafana repo\n\n\n\n  helm repo add grafana https://grafana.github.io/helm-charts\n\n\n\n  Create config file\n\n\n\n  You need to modify pvc section:\n\n  storageClassName &gt; your default storage class name available in your k8s environment\n\n  existingClaim &gt; you need it if you have more than one pvc for grafana\n\n\ncat &gt; grafana.yaml &lt;&lt;EOF\n\n## Expose the grafana service to be accessed from outside the cluster\n(LoadBalancer service).\n\n## or access it from within the cluster (ClusterIP service). Set the\nservice type and the port to serve it.\n\n## ref: http://kubernetes.io/docs/user-guide/services/\n\n##\n\nservice:\n\nenabled: true\n\ntype: LoadBalancer\n\nport: 80\n\ntargetPort: 3000\n\nportName: service\n\nselector:\n\napp.kubernetes.io/name: grafana\n\napp.kubernetes.io/instance: grafana\n\n## Enable persistence using Persistent Volume Claims\n\n## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n\n##\n\npersistence:\n\ntype: pvc\n\nenabled: enable\n\nstorageClassName: default\n\naccessModes:\n\n- ReadWriteOnce\n\nsize: 5Gi\n\nfinalizers:\n\n- kubernetes.io/pvc-protection\n\n# selectorLabels: {}\n\n# subPath: \"\"\n\n# existingClaim: grafana\n\nEOF\n\n\n  Install Grafana in Kubernetes environment using helm\n helm install grafana grafana/grafana -nmemsql -f grafana.yaml\n\n\nInstall the Plugin\n\n\n  \n    Get grafana pod name\n kubectl get po -nmemsql\n  \n  \n    Install the plugin\n\n    a.  Piechart Panel\n  \n\n\n\n  kubectl exec -it &lt;pod-name&gt; -n&lt;namespace&gt; -- grafana-cli plugins\ninstall grafana-piechart-panel\n\n\nb.  Multibar Graph Panel\n\n\n  kubectl exec -it &lt;pod-name&gt; -n&lt;namespace&gt; -- grafana-cli\n--pluginUrl\nhttps://github.com/CorpGlory/grafana-multibar-graph-panel/archive/0.2.5.zip\nplugins install multibar-graph-panel\n\n  Sample output:\n\n\n![](../../assets/img/docs_deploy_aws_media/media/image39.png)\n\n\n\n  Delete grafana pod to restart the service\n\n\n\n  kubectl delete &lt;pod-name&gt; -n&lt;namespace&gt;\n\n\nAccess\n\nAdd the grafana monitoring datasource\n\n\n  Get the external IP &amp; port\n\n\n\n  kubectl get svc grafana -n&lt;namespace&gt;\n\n  Sample output:\n\n\n![](../../assets/img/docs_deploy_aws_media/media/image43.png)\n\n\n\n  Access the url\n\n\n\n  http://&lt;external-ip&gt;:&lt;nodeport&gt;\n\n  Sample: http://13.127.222.56:30153/\n\n\n\n  Get the admin’s password\n\n\n\n  kubectl get secret -n&lt;namespace&gt; grafana -o\njsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n\n  Sample output:\n\n\n![](../../assets/img/docs_deploy_aws_media/media/image32.png)\n\n\n\n  \n    Login\n Log in using admin as the username and the password.\n\n    \n  \n\n\nDatasource\n\n\n  \n    Add a monitoring MySQL data source with the following settings.\n\n    a.  Click the gear icon in the side menu, and then click Datasource. Then Click Add data source button.\n\n    {width=”4.941772747156605in” height=”3.6718755468066493in”}\n  \n\n\nb.  Type mysql on filter box, then click Select button.\n\n![](../../assets/img/docs_deploy_aws_media/media/image23.png)\n\n\nc.  FIll out the form.\n\n\n  Data source name: monitoring\n\n  Data source type: mysql\n\n  Data source host: &lt;metrics-cluster-master-aggregator-host&gt;\n\n  Data source port: 3306\n\n  Database name: metrics\n\n  User: dbmon\n\n  Password: &lt;secure-password-or-blank-if-none&gt;\n\n\n![](../../assets/img/docs_deploy_aws_media/media/image28.png)\n\n\nd.  Click Save &amp; test button.\n\n![](../../assets/img/docs_deploy_aws_media/media/image27.png)\n\n\n###\n\nDashboard\n\n\n  \n    Download the cluster monitoring dashboards from SingleStore and extract the downloaded file cluster-monitoring-dashboards-73.zip\n\n    \n  \n  \n    Import the dashboards into Grafana.\n  \n\n\n&lt;!-- --&gt;\n\na.  Click the + icon in the side menu, and then click Import.\n\nb.  From here you can upload a dashboard JSON file.\n\nc.  Click Upload JSON file button\n\n![](../../assets/img/docs_deploy_aws_media/media/image29.png)\n\n\n\n  \n    Click Import button\n\n    \n  \n  \n    View the Dashboards\n When all cluster monitoring components are installed, configured, and running, the Grafana dashboards can be used to monitor SingleStore DB cluster health over time.\n {width=”5.830295275590551in” height=”3.682292213473316in”}\n  \n\n\nRollback/Cleanup\n\nAKS Cluster\n\nSkip this step if you want to retain the AKS cluster for another\napplication\n\n\n  \n    Login to Azure portal.\n  \n  \n    In Home page search for Kubernetes service.\n  \n\n\n\n\n\n  Click on the Kubernetes Service and choose the Kubernetes Cluster you want to delete.\n\n\n\n\n\n  Click on the Delete option on the right side top as shown in figure.\n\n\n\n\n\n  Confirm the delete operation by pressing \"Yes\" button.\n\n\n\n\n\n  Check the Delete operation status.\n\n\n\n\nSingleStore DB\n\nSkip this step if you did the previous step (delete AKS cluster).\n\n\n  Delete all Kubernetes object using helm command\n\n\n\n  helm delete singlestore -n&lt;namespaces&gt;\n\n\n\n  Clean up the PersistentVolumeClaim\n\n\n\n  Run the below command:\n\n  kubectl delete pvc --all -n&lt;namespaces&gt;\n\n\n\n  Clean up the CustomResourceDefinition\n\n\n\n  Run the below command:\n\n  kubectl delete crd MemsqlCluster\n\n\n\n  Clean up the Namespaces\n\n\n\n  Run the below command:\n\n  kubectl delete ns &lt;namespaces&gt;\n\n\nUseful Command\n\nKubernetes\n\nKubernetes object\n\nstatefulsets, deployment, pod, memsqlcluster, pvc, pv\n\nkubectl command:\n\nGo to inside pod:\n\nkubectl exec -it [pod name] -n &lt;namespace&gt; -- bash\n\nOther:\n\nkubectl get statefulsets\n\nkubectl get deploy -n memsql\n\nkubectl describe deploy [deployment name] -n memsql\n\nkubectl get pod -n memsql\n\nkubectl describe pod [pod name] -n memsql\n\nkubectl get memsqlcluster -n memsql\n\nkubectl describe memsqlcluster [memsqlcluster name] -n memsql\n\nkubectl get pvc -n memsql\n\nkubectl describe pvc [pvc name] -n memsql\n\nkubectl get pv -n memsql\n\nkubectl describe pv [pv name] -n memsql\n\nkubectl logs [pod name] -n memsql\n\nkubectl logs -f [pod name] -n&lt;namespace&gt;\n\nSingleStore\n\nsinglestore command need to run inside the master pod:\n\nCheck license:\n\nmemsqlctl show-license\n\nList MemSQL Nodes on the local machine:\n\nmemsqlctl list-nodes\n\nList aggregator node:\n\nmemsqlctl show-aggregators\n\nList leaves node:\n\nmemsqlctl show-leaves\n\nDeployment Artifacts\nDeployment related files can be found at https://github.com/sdb-cloud-ops/AWS\n\n  k8s operator yamls\n  terraform scripts"
					}
					
				
		
				
					,
					
					"docs-deploying-azure-deploying": {
						"id": "docs-deploying-azure-deploying",
						"title": "Deploy SingleStore with Azure Kubernetes Service",
						"categories": "",
						"url": " /docs/Deploying/Azure/deploying",
						"content": "Deploy SingleStore with Azure Kubernetes Service\n\nPrerequisites\n\nMake sure you have kubectl and Azure CLI installed\n\nObtain a SingleStore license from the SingleStore Customer Portal.\n\nUse the az login command to initially login to Azure\n\nCreating and connecting to the Kubernetes cluster\n\nWe will use the az aks create command to create our sdb-cluster using the following flags:\n\n--name and --resource-group are required\n\n--node-count\n--node-vm-size\n--enable-cluster-autoscaler\n--min-count\n--max-count\n--network-plugin\n--network-policy\n\n\nExample command\naz aks create --name sdb-cluster --resource-group singlestore --node-count 4 --node-vm-size standard_d4s_v3 --enable-cluster-autoscaler --min-count 1 --max-count 12 --network-plugin azure --network-policy calico\n\n\n  Azure CNI and Calico are required for SingleStore\n\n\nTo set your account\naz account set --subscription &lt;subscription-id&gt;\n\nTo get your credentials\nConfigure kubectl to connect to your Kubernetes cluster using the az aks get-credentials command. The following command downloads credentials and configures the Kubernetes CLI to use them.\n\naz aks get-credentials --resource-group singlestore --name sdb-cluster\n\nVerify\nIf you have kubectl correctly installed, you should now be able to run kubectl get ns without errors.\n\nDeploy SingleStore on Kubernetes\n\nDownload the Operator image (optional if not pulling from deployment.yaml)\n\nThe memsql/operator can be pulled from Docker Hub\n\nExample with Docker\ndocker pull memsql/operator:1.2.5-83e8133a\n\n\nCreate the Object Definition files\n\nDownload the object definition files from our site, and save them in an accessible directory. The rbac.yaml and the memsql-cluster-crd.yaml do not require edits.\n\nEdit the deployment.yaml and reference the image you downloaded, or you can pull it automatically.\n\ndeployment.yaml\n\nExample spec\n    spec:\n      serviceAccountName: memsql-operator\n      containers:\n        - name: memsql-operator\n          image: memsql/operator:1.2.5-83e8133a\n          imagePullPolicy: IfNotPresent\n\n\nmemsql-cluster.yaml\n\nIn order to edit the memsql-cluster.yaml you will need your license key from the customer portal and a hashed version of a secure password for the admin user.\n\nIn order to hash a secure password, we have an example python script:\nfrom hashlib import sha1\nprint(\"*\" + sha1(sha1('secretpass'.encode('utf-8')).digest()).hexdigest().upper())\n\nHow to use the script\n\n  Save the script in a file named hash_password.py, replacing secretpass with a secure password. Make sure it is executable with chmod +x hash_password.py, then run the script with python3 hash_password.py. It will print the hashed password to the command line, where you can copy and paste directly to the memsql-cluster.yaml file.\n\n\nCreate a namespace for the Kubernetes Objects\nkubectl create ns singlestore\n\nAdd the namespace to the object metadata\napiVersion: memsql.com/v1alpha1\nkind: MemsqlCluster\nmetadata:\n  name: memsql-cluster\n  namespace: singlestore\n\nAdd the licence_key, hashed_password keeping the quotes, and use the latest memsql/node image\nspec:\n  license: &lt;license_key&gt;\n  adminHashedPassword: \"&lt;hashed_password&gt;\"\n  nodeImage:\n    repository: memsql/node\n    tag: latest\n\nChange the redundancy level to 2\n  redundancyLevel: 2\n\nChange the storage class to default within the aggregatorSpec:\nstorageClass: default\n\n\nThe rest of the edits to the aggregatorSpec and the leafSpec are custom depending on how many aggregators and leafs you want and how many resources you want allocated to them. Save all yaml files to an accessible directory.\n\nCreate the Kubernetes Objects\n\nCreate the role-based access control\nkubectl -n singlestore create -f rbac.yaml\n\nCreate the memsql custom resource definition\nkubectl -n singlestore create -f memsql-cluster-crd.yaml\n\nCreate the Operator deployment\nkubectl -n singlestore create -f deployment.yaml\n\nNow you should be able to run kubectl -n singlestore get pods and see that the Operator pod is running.\nNAME                               READY   STATUS    RESTARTS   AGE\nmemsql-operator-79874797f4-9t47r   1/1     Running   0          2m50s\n\nCreate the memsql custom resource\nkubectl -n singlestore create -f memsql-cluster.yaml\n\nAfter a couple minutes, run kubectl -n singlestore get pods again to verify the aggregator and leaf nodes all started and have a status of Running.\n\nIf you see no pods are in the Running state, check the Operator logs by running\nkubectl -n singlestore logs deployment memsql-operator then look at the various objects to see what is failing.\n\nNow you can run kubectl -n singlestore get memsql to see the custom resource that was created.\nNAME             AGGREGATORS   LEAVES   REDUNDANCY LEVEL   AGE\nmemsql-cluster   1             1        2                  30h\n\nVerify the cluster\nkubectl get memsql memsql-cluster -o=jsonpath='{.status.phase}{\"\\n\"}' -n singlestore\n\nThe SingleStore DB server deployment is complete when Running is displayed after running the above commands.\n\nConnect to the database\nInstall the SingleStore Client\n\nAfter the deployment completes, run the following command to display the two SingleStore DB service endpoints that are created during the deployment\nkubectl -n singlestore get services\n\nThe svc-&lt;cluster-name&gt;-ddl and svc-&lt;cluster-name&gt;-dml service endpoints can be used to connect to SingleStore DB using a MySQL compatible client. Note that svc--dml only exists if  memsqlCluster.aggregatorSpec.count is greater than 1.\nNAME                     TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)          AGE\nsvc-memsql-cluster       ClusterIP      None             &lt;none&gt;          3306/TCP         30h\nsvc-memsql-cluster-ddl   LoadBalancer   &lt;ip-address&gt;     &lt;ip-address&gt;    3306:30907/TCP   30h\n\nThe IP address under EXTERNAL-IP in the svc-memsql-cluster-ddl row is the one that you will use to connect to your database. You will use the admin user and the un-hashed password with port 3306 to connect.\n\nExample connection command\nsinglestore -h&lt;ip-address&gt; -uadmin -P3306 -p&lt;secretpass&gt;\n\nRefer to Data Definition Language DDL and Data Manipulation Language DML for more information.\n\nCheck status of aggregators and leafs\nshow aggregators;\n\nshow leaves;\n\n\nThis concludes setting up SingleStore with Azure Kubernetes Engine.\n\nDeployment Artifacts\nDeployment related files can be found at https://github.com/sdb-cloud-ops/Azure\n\n  k8s operator yamls\n  terraform scripts"
					}
					
				
		
				
					,
					
					"docs-deploying-gcp-deploying": {
						"id": "docs-deploying-gcp-deploying",
						"title": "Deploy SingleStore with Google Cloud Kubernetes Engine",
						"categories": "",
						"url": " /docs/Deploying/GCP/deploying",
						"content": "Deploy SingleStore with Google Cloud Kubernetes Engine\n\nPrerequisites\n\nMake sure you have kubectl and Google Cloud SDK installed.\n\nUse the gcloud tool to configure the following default settings: your default project, compute zone, and compute region.\n\nCreating and connecting to the Kubernetes cluster\n\nWe will use the gcloud containers create command to create our sdb-cluster using the following optional flags:\n\n[--network=NETWORK]\n[--subnetwork=SUBNETWORK]\n[--region=REGION | --zone=ZONE, -z ZONE]\n[--image-type=IMAGE_TYPE]\n[--machine-type=MACHINE_TYPE, -m MACHINE_TYPE]\n[--node-locations=ZONE,[ZONE,…]]\n[--enable-autoscaling]\n[--num-nodes NUM_NODES]\n[--min-nodes MIN_NODES]\n[--max-nodes MAX_NODES]\n\n\nExample command\ngcloud container clusters create sdb-cluster --region=us-east4 --node-locations=us-east4-a --machine-type=n2-standard-16 --image-type=cos --enable-ip-alias --create-subnetwork name=sdb-subnet --enable-autoscaling --num-nodes=4 --min-nodes=0 --max-nodes=10\n\n\n  Note: Warnings are expected in the output at this time.\n\n\nRun the gcloud container clusters get-credentials command to connect to your Kubernetes cluster.\n\nExample command\ngcloud container clusters get-credentials sdb-cluster --region us-east4 --project &lt;project-name&gt;\n\nIf you have kubectl correctly installed, you should now be able to run kubectl get ns without errors.\n\nDeploy SingleStore on Kubernetes\n\nDownload the Operator image (optional if not pulling from deployment.yaml)\n\nThe memsql/operator can be pulled from Docker Hub or from the Red Hat container registry.\n\nExample with Docker\ndocker pull memsql/operator:1.2.5-83e8133a\n\n\nCreate the Object Definition files\n\nDownload the object definition files from our site, and save them in an accessible directory. The rbac.yaml and the memsql-cluster-crd.yaml do not require edits.\n\nEdit the deployment.yaml and reference the image you downloaded, or you can pull it automatically.\n\ndeployment.yaml\n\nExample spec\n    spec:\n      serviceAccountName: memsql-operator\n      containers:\n        - name: memsql-operator\n          image: memsql/operator:1.2.5-83e8133a\n          imagePullPolicy: IfNotPresent\n\n\nmemsql-cluster.yaml\n\nIn order to edit the memsql-cluster.yaml you will need your license key from the customer portal and a hashed version of a secure password for the admin user.\n\nIn order to hash a secure password, we have an example python script:\nfrom hashlib import sha1\nprint(\"*\" + sha1(sha1('secretpass'.encode('utf-8')).digest()).hexdigest().upper())\n\nHow to use the script\n\n  Save the script in a file named hash_password.py, replacing secretpass with a secure password. Make sure it is executable with chmod +x hash_password.py, then run the script with python3 hash_password.py. It will print the hashed password to the command line, where you can copy and paste directly to the memsql-cluster.yaml file.\n\n\nCreate a namespace for the Kubernetes Objects\nkubectl create ns singlestore\n\nAdd the namespace to the object metadata\napiVersion: memsql.com/v1alpha1\nkind: MemsqlCluster\nmetadata:\n  name: memsql-cluster\n  namespace: singlestore\n\nAdd the licence_key, hashed_password keeping the quotes, and use the latest memsql/node image\nspec:\n  license: &lt;license_key&gt;\n  adminHashedPassword: \"&lt;hashed_password&gt;\"\n  nodeImage:\n    repository: memsql/node\n    tag: latest\n\nChange the redundancy level to 2\n  redundancyLevel: 2\n\nThe rest of the edits to the aggregatorSpec and the leafSpec are custom depending on how many aggregators and leafs you want and how many resources you want allocated to them. Save all yaml files to an accessible directory.\n\nCreate the Kubernetes Objects\n\nCreate the role-based access control\nkubectl -n singlestore create -f rbac.yaml\n\nCreate the memsql custom resource definition\nkubectl -n singlestore create -f memsql-cluster-crd.yaml\n\nCreate the Operator deployment\nkubectl -n singlestore create -f deployment.yaml\n\nNow you should be able to run kubectl -n singlestore get pods and see that the Operator pod is running.\nNAME                               READY   STATUS    RESTARTS   AGE\nmemsql-operator-79874797f4-9t47r   1/1     Running   0          2m50s\n\nCreate the memsql custom resource\nkubectl -n singlestore create -f memsql-cluster.yaml\n\nNow you can run kubectl -n singlestore get memsql to see the custom resource that was created.\nNAME             AGGREGATORS   LEAVES   REDUNDANCY LEVEL   AGE\nmemsql-cluster   1             1        2                  30h\n\nConnect to the database\n\nRun kubectl -n singlestore get services in order to get the ddl endpoint\nNAME                     TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)          AGE\nsvc-memsql-cluster       ClusterIP      None             &lt;none&gt;          3306/TCP         30h\nsvc-memsql-cluster-ddl   LoadBalancer   &lt;ip-address&gt;     &lt;ip-address&gt;    3306:30907/TCP   30h\n\nThe IP address under EXTERNAL-IP in the svc-memsql-cluster-ddl row is the one that you will use to connect to your database. You will use the admin user and the un-hashed password with port 3306 to connect.\n\nExample connection command\nmysql -h&lt;ip-address&gt; -uadmin -P3306 -p&lt;secretpass&gt;\n\nThis concludes setting up SingleStore with Google Cloud Kubernetes Engine.\n\nDeployment Artifacts\nDeployment related files can be found at https://github.com/sdb-cloud-ops/GCP\n\n  k8s operator yamls\n  terraform scripts"
					}
					
				
		
				
					,
					
					"docs-deploying-deploying-home": {
						"id": "docs-deploying-deploying-home",
						"title": "SingleStore Installation",
						"categories": "",
						"url": " /docs/Deploying/deploying_home",
						"content": "Deploy SingleStore on AWS\n\nDeploy SingleStore on GCP\n\nDeploy SingleStore on Azure"
					}
					
				
		
				
					,
					
					"docs-disasterrecovery-disaster-recovery": {
						"id": "docs-disasterrecovery-disaster-recovery",
						"title": "Disaster Recovery page",
						"categories": "",
						"url": " /docs/DisasterRecovery/disaster_recovery",
						"content": "Content yet to be filled"
					}
					
				
		
				
					,
					
					"docs-monitoring-monitoring": {
						"id": "docs-monitoring-monitoring",
						"title": "Monitoring page",
						"categories": "",
						"url": " /docs/Monitoring/monitoring",
						"content": "Monitoring SingleStore on Kubernetes\nThis document will show you how to install Prometheus, Grafana, and Grafana Loki on a GKE cluster. It will also provide basics on Grafana use including adding the Loki data source to Grafana in order to view and query logs and exploring the default dashboards provided by the kube-prometheus stack.\n\nPrometheus\n\n  Note we are using the kube-prometheus stack. Please refer to this doc for reference.\n\n\nInstall dependencies\n\n  On MacOS you can use brew to install these packages\n\n\n\n  \n    jsonnet-bundler\n  \n  \n    jsonnet\n  \n  \n    go\n  \n  gojsontoyaml library\n      go install github.com/brancz/gojsontoyaml@latest\n  go install github.com/google/go-jsonnet/cmd/jsonnet@latest\n    \n  \n  wget\n\n\nCreate the install directory\nmkdir kube-prometheus\n\ncd kube-prometheus\n\n\nInitialize jb and install kube-prometheus\njb init\n\njb install github.com/prometheus-operator/kube-prometheus/jsonnet/kube-prometheus@main\n\n\nPull the example.jsonnet and build.sh files\nwget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/example.jsonnet -O example.jsonnet\n\nwget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/build.sh -O build.sh\n\n\nUpdate jb\njb update\n\n\nMake build.sh executable\nchmod +x build.sh\n\n\n\n  Note: If you need to update GOPATH in the build.sh file edit line as below:\n\n  jsonnet -J vendor -m manifests \"${1-example.jsonnet}\" | xargs -I{} sh -c 'cat {} | $(go env GOPATH)/bin/gojsontoyaml &gt; {}.yaml; rm -f {}' -- {}\n\n\n\nBuild the customization file\n\nvim memsql.jsonnet\n\nlocal kp = (import 'kube-prometheus/main.libsonnet') + {\n  values+:: {\n    common+: {\n      namespace: 'monitoring',\n    },\n    prometheus+:: {\n      namespaces+: ['singlestore'],\n    },\n  },\n  memsqlMetrics: {\n    serviceMonitorMyNamespace: {\n      apiVersion: 'monitoring.coreos.com/v1',\n      kind: 'ServiceMonitor',\n      metadata: {\n        name: 'memsql-metrics',\n        namespace: 'singlestore',\n      },\n      spec: {\n        endpoints: [\n          {\n            path: '/metrics',\n            targetPort: 'metrics',\n          },\n        ],\n        jobLabel: 'app.kubernetes.io/name',\n        selector: {\n          matchLabels: {\n            'app.kubernetes.io/name': 'memsql-cluster',\n            'app.kubernetes.io/component': 'cluster',\n          },\n        },\n      },\n    },\n  },\n  memsqlClusterMetrics: {\n    serviceMonitorMyNamespace: {\n      apiVersion: 'monitoring.coreos.com/v1',\n      kind: 'ServiceMonitor',\n      metadata: {\n        name: 'memsql-cluster-metrics',\n        namespace: 'singlestore',\n      },\n      spec: {\n        endpoints: [\n          {\n            path: '/cluster-metrics',\n            targetPort: 'metrics',\n          },\n        ],\n        jobLabel: 'app.kubernetes.io/name',\n        selector: {\n          matchLabels: {\n            'app.kubernetes.io/name': 'memsql-cluster',\n            'app.kubernetes.io/component': 'master',\n          },\n        },\n      },\n    },\n  },\n};\n\n{ 'setup/0namespace-namespace': kp.kubePrometheus.namespace } +\n{\n  ['setup/prometheus-operator-' + name]: kp.prometheusOperator[name]\n  for name in std.filter((function(name) name != 'serviceMonitor' &amp;&amp; name != 'prometheusRule'), std.objectFields(kp.prometheusOperator))\n} +\n// serviceMonitor and prometheusRule are separated so that they can be created after the CRDs are ready\n{ 'prometheus-operator-serviceMonitor': kp.prometheusOperator.serviceMonitor } +\n{ 'prometheus-operator-prometheusRule': kp.prometheusOperator.prometheusRule } +\n{ 'kube-prometheus-prometheusRule': kp.kubePrometheus.prometheusRule } +\n{ ['alertmanager-' + name]: kp.alertmanager[name] for name in std.objectFields(kp.alertmanager) } +\n{ ['blackbox-exporter-' + name]: kp.blackboxExporter[name] for name in std.objectFields(kp.blackboxExporter) } +\n{ ['grafana-' + name]: kp.grafana[name] for name in std.objectFields(kp.grafana) } +\n{ ['kube-state-metrics-' + name]: kp.kubeStateMetrics[name] for name in std.objectFields(kp.kubeStateMetrics) } +\n{ ['kubernetes-' + name]: kp.kubernetesControlPlane[name] for name in std.objectFields(kp.kubernetesControlPlane) }\n{ ['node-exporter-' + name]: kp.nodeExporter[name] for name in std.objectFields(kp.nodeExporter) } +\n{ ['prometheus-' + name]: kp.prometheus[name] for name in std.objectFields(kp.prometheus) } +\n{ ['prometheus-adapter-' + name]: kp.prometheusAdapter[name] for name in std.objectFields(kp.prometheusAdapter) } +\n{ ['memsql-metrics-' + name]: kp.memsqlMetrics[name] for name in std.objectFields(kp.memsqlMetrics) } +\n{ ['memsql-cluster-metrics-' + name]: kp.memsqlClusterMetrics[name] for name in std.objectFields(kp.memsqlClusterMetrics) }\n\n\n  Note: Only change the namespace inside of this file where it says singlestore to the namespace that you have SingleStore deployed.\n\n\nApply the build to the manifest files\n./build.sh memsql.jsonnet\n\n\nCreate the kubernetes objects\nkubectl apply --server-side -f manifests/setup\n\nkubectl apply -f manifests/\n\n\nTo access Prometheus locally\nRun the command below and then go to http://localhost:9090 in your web browser to access Prometheus\nkubectl --namespace monitoring port-forward svc/prometheus-k8s 9090\n\nGrafana Loki\n\n\n  Note: further installation methods are provided in the Reference section below\n\n\nGKE\n\nFor this installation, we will be creating a Google cloud storage bucket for our backend log store.\n\nPrerequisites\n\nInstall Tanka\n\n  MacOS users can use brew install tanka\n\n\nCreate a Google cloud bucket and service account\nGo to the Google cloud storage browser, and click create a bucket. We will need to create a service account in order for Loki to access the bucket, so next, go to IAM &amp; Admin &gt; Service Accounts, and create a service account. I gave my service account Storage Admin and Storage Object Admin roles. Once the service account is created, click into the account and go to the Keys section. There you can click add key and make sure to save the downloaded file.\n\nCreate a secret based off the google key\nkubectl -n monitoring create secret generic google-key --from-file=key.json=&lt;path/to/downloaded/key.json&gt;\n\n\nInstall Grafana Loki with Tanka\n\n  Please refer to this documentation for reference.\n\n\nCreate a local directory for the Loki environment\nmkdir loki\n\ncd loki\n\nInitialize Tanka\ntk init\n\nFind your kubernetes api server\nvim ~/.kube/config\n\nFind the - cluster section of the cluster you are using, and locate the server: section. It will be right below the cluster certificate, and right above the cluster name. This is your kubernetes api server.\ntk env add environments/loki --namespace=monitoring --server=&lt;Kubernetes API server&gt;\n\nDownload and install the Loki and Promtail module using jb\njb install github.com/grafana/loki/production/ksonnet/loki@main\njb install github.com/grafana/loki/production/ksonnet/promtail@main\n\nCreate the htpasswd file\nThis example creates a .loki file with the loki username\nhtpasswd -c .loki loki\n\nIt will prompt for a new password. Enter this twice. Then view the contents of the file for input into the htpasswd_contents: section of the main.jsonnet\n\nEdit the environments/loki/main.jsonnet file\nMake sure to replace the commented items with values that are specific to your deployment.\nvim environments/loki/main.jsonnet\n\nlocal gateway = import 'loki/gateway.libsonnet';\nlocal loki = import 'loki/loki.libsonnet';\nlocal promtail = import 'promtail/promtail.libsonnet';\nlocal k = import 'ksonnet-util/kausal.libsonnet';\nlocal gcsBucket = import 'gcsBucket.libsonnet';\n\nloki + promtail + gateway + gcsBucket {\n\n  _config+:: {\n    namespace: 'loki',\n    htpasswd_contents: 'loki:$apr1$i.G5yqP8$LGo1ANcwfGH87unfpIr6m.', //content of your .loki file created above\n\n    // GCS variables -- Remove if not using gcs\n    storage_backend: 'gcs',\n    gcs_bucket_name: 'sdb-loki', //name of the google bucket\n\n    //Set this variable based on the type of object storage you're using.\n    boltdb_shipper_shared_store: 'gcs',\n\n    //Update the object_store and from fields\n    loki+: {\n      schema_config: {\n        configs: [{\n          from: '2022-02-02', //set this date to the date exactly 2 weeks ago from today\n          store: 'boltdb-shipper',\n          object_store: 'gcs',\n          schema: 'v11',\n          index: {\n            prefix: '%s_index_' % $._config.table_prefix,\n            period: '%dh' % $._config.index_period_hours,\n          },\n        }],\n      },\n    },\n\n    //Update the container_root_path if necessary\n    promtail_config+: {\n      clients: [{\n        scheme:: 'http',\n        hostname:: 'gateway.%(namespace)s.svc' % $._config,\n        username:: 'loki',\n        password:: 'loki', //same password that was used for the loki username in the htpasswd file\n        container_root_path:: '/var/lib/docker',\n      }],\n    },\n\n    replication_factor: 3,\n    consul_replicas: 1,\n  },\n}\n\nCreate a new file in the lib directory that edits the yaml files for GKE specific variables\nvim lib/gcsBucket.libsonnet\n\n//gcsBucket.libsonnet\n\n// Import libs\n{\n  local volumeMount = $.core.v1.volumeMount,\n  local container = $.core.v1.container,\n  local statefulSet = $.apps.v1.statefulSet,\n  local volume = $.core.v1.volume,\n  local secret = $.core.v1.secret,\n  local deployment = $.apps.v1.deployment,\n  local pvc = $.core.v1.persistentVolumeClaim,\n  local spec = $.core.v1.spec,\n  local storageClass = $.storage.v1.storageClass,\n\n  //distributor\n  distributor_container+::\n    container.withEnvMap({GOOGLE_APPLICATION_CREDENTIALS:'/var/secrets/google/key.json'}),\n\n  distributor_deployment+:\n    $.util.secretVolumeMount('google', '/var/secrets/google'),\n\n  distributor_args+:: {\n    'config.expand-env':'true',\n  },\n\n\n  //querier\n  querier_container+::\n    container.withEnvMap({GOOGLE_APPLICATION_CREDENTIALS:'/var/secrets/google/key.json'}),\n\n  querier_statefulset+::\n    $.util.secretVolumeMount('google', '/var/secrets/google'),\n\n  querier_args+:: {\n    'config.expand-env':'true',\n  },\n\n  querier_data_pvc+:\n    pvc.mixin.spec.withStorageClassName('standard'),\n\n\n  //ingester\n  ingester_container+::\n    container.withEnvMap({GOOGLE_APPLICATION_CREDENTIALS:'/var/secrets/google/key.json'}),\n\n  ingester_statefulset+:\n    $.util.secretVolumeMount('google', '/var/secrets/google'),\n\n  ingester_args+:: {\n    'config.expand-env':'true',\n  },\n\n  ingester_data_pvc+::\n    pvc.mixin.spec.withStorageClassName('standard'),\n\n  ingester_wal_pvc+::\n    pvc.mixin.spec.withStorageClassName('standard'),\n\n  //compactor\n  compactor_container+::\n    container.withEnvMap({GOOGLE_APPLICATION_CREDENTIALS:'/var/secrets/google/key.json'}),\n\n  compactor_statefulset+:\n    $.util.secretVolumeMount('google', '/var/secrets/google'),\n\n  compactor_args+:: {\n    'config.expand-env':'true',\n  },\n\n  compactor_data_pvc+::\n    pvc.mixin.spec.withStorageClassName('standard'),\n\n\n  //table-manager\n  table_manager_container+::\n    container.withEnvMap({GOOGLE_APPLICATION_CREDENTIALS:'/var/secrets/google/key.json'}),\n\n  table_manager_deployment+:\n    $.util.secretVolumeMount('google', '/var/secrets/google'),\n\n  table_manager_args+:: {\n    'config.expand-env':'true',\n  },\n\n\n  //query-frontend\n  query_frontend_container+::\n    container.withEnvMap({GOOGLE_APPLICATION_CREDENTIALS:'/var/secrets/google/key.json'}),\n\n  query_frontend_deployment+:\n    $.util.secretVolumeMount('google', '/var/secrets/google'),\n\n  query_frontend_args+:: {\n    'config.expand-env':'true',\n  },\n\n}\n\nUse tanka to create the yaml files\ntk export manifests environments/loki\n\nThis will give you a manifests directory of yaml files for review.\n\nApply the yaml files to the kubernetes cluster\nkubectl --namespace monitoring create -f manifests\n\n\nAdd the Loki data source to Grafana\n\nRun the command below and then go to http://localhost:3000 in your web browser to access Grafana\nkubectl --namespace monitoring port-forward svc/grafana 3000:3000\n\n\nGo to Configuration &gt; Data Sources\n\n\nClick add data source and then type in ‘Loki’\n\n\nEnter the Loki URL\n\n\nClick ‘Save &amp; Test’ and you should see the message below\n\n\nNow you can explore SingleStore logs through Grafana + Loki\n\n\nExpand the Log Browser in order to have a helpful UI\n\n\nExample query with SingleStore logs\n\n\n\n  Here is further information on LogQL Queries\n\n\nAccess Grafana Dashboards\n\n  Kube-prometheus added 24 dashboards to Grafana to get you started with dashboard monitoring\n\n\nClick on Dashboards &gt; Browse\n\n\nExpand the Default folder\n\n\nClick on a dashboard to try it out!\n\nNow we have installed a monitoring stack consisting of Prometheus and Grafana Loki. Further reading:\nDashboard overview,\nGrafana alerting\n\nReference\n\nFurther installation methods of Grafana Loki\n\nInstall Grafana Loki with Helm with a persistant volume claim\n\n\n  Further installation methods are detailed here\n\n\nPrereqs: Install Helm, then run the commands below\n\nhelm repo add grafana https://grafana.github.io/helm-charts\n\nhelm repo update\n\n\nInstall Loki + Promtail\nhelm upgrade --install loki --namespace=monitoring grafana/loki-stack  --set loki.persistence.enabled=true,loki.persistence.storageClassName=standard,loki.persistence.size=5Gi\n\n\n  Note: you can change the size of the volume claim\n\n\nGrafana is accessed the same way, but not that the Loki URL is http://loki:3100 when adding the Loki datasource to Grafana after installing with Helm"
					}
					
				
		
				
					,
					
					"docs-scaling-scaling": {
						"id": "docs-scaling-scaling",
						"title": "Scaling page",
						"categories": "",
						"url": " /docs/Scaling/scaling",
						"content": "Content yet to be filled"
					}
					
				
		
		
				
					,
					
					"2019-hello-world": {
						"id": "2019-hello-world",
						"title": "Two Thousand Nineteen",
						"categories": "jekyll, update",
						"url": " /2019/hello-world/",
						"content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
					}
					
				
		
				
					,
					
					"2019-welcome": {
						"id": "2019-welcome",
						"title": "Welcome to Docsy Jekyll",
						"categories": "jekyll, update",
						"url": " /2019/welcome/",
						"content": "You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\n\n\n\nTo add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.\n\nJekyll also offers powerful support for code snippets:\n\ndef print_hi(name)\n  puts \"Hi, #{name}\"\nend\nprint_hi('Tom')\n#=&gt; prints 'Hi, Tom' to STDOUT.\n\nCheck out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk."
					}
					
				
		
		
				
					,
					
					"404-html": {
						"id": "404-html",
						"title": "",
						"categories": "",
						"url": " /404.html",
						"content": "404\n\n  Page not found :(\n  The requested page could not be found."
					}
					
				
		
				
					,
					
					"about": {
						"id": "about",
						"title": "About",
						"categories": "",
						"url": " /about/",
						"content": "This is the base Jekyll theme. You can find out more info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at jekyllrb.com\n\nYou can find the source code for Minima at GitHub:\njekyll /\nminima\n\nYou can find the source code for Jekyll at GitHub:\njekyll /\njekyll"
					}
					
				
		
				
					,
					
					"about": {
						"id": "about",
						"title": "About",
						"categories": "",
						"url": " /about/",
						"content": "About\n\nThis is a starter template for a Docsy jekyll theme, based\non the Beautiful Docsy that renders with Hugo. This version is intended for\nnative deployment on GitHub pages. See the respository for more details.\n\nSupport\n\nIf you need help, please don’t hesitate to open an issue."
					}
					
				
		
				
					,
					
					"archive": {
						"id": "archive",
						"title": "Articles",
						"categories": "",
						"url": " /archive/",
						"content": "News Archive\n\n2019\n\n\n  Jun 29, 2019: Two Thousand Nineteen\n  \n\n\n\n  Jun 28, 2019: Welcome to Docsy Jekyll"
					}
					
				
		
				
					,
					
					"docs": {
						"id": "docs",
						"title": "Technical Guides",
						"categories": "",
						"url": " /docs/",
						"content": "Technical Guides\n\nWelcome to the Cloud-ops Documentation pages! Here you can quickly jump to a \nparticular page.\n\n\n    \n            \n    \n    Deploy SingleStore with Amazon Web Service\n    \n            \n    \n    Deploy SingleStore with Azure Kubernetes Service\n    \n            \n    \n    Deploy SingleStore with Google Cloud Kubernetes Engine\n    \n            \n    \n    SingleStore Installation\n    Installing SingleStore on public cloud providers\n            \n    \n    Disaster Recovery page\n    \n            \n    \n    Monitoring page\n    \n            \n    \n    Scaling page"
					}
					
				
		
				
					,
					
					"feed-xml": {
						"id": "feed-xml",
						"title": "",
						"categories": "",
						"url": " /feed.xml",
						"content": "Cloud-ops\n    \n    http://localhost:4000/cloud-ops/\n    \n    Wed, 27 Apr 2022 18:21:33 +0530\n    Wed, 27 Apr 2022 18:21:33 +0530\n    Jekyll v3.9.0\n    \n      \n        Two Thousand Nineteen\n        &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&lt;/p&gt;\n\n        Sat, 29 Jun 2019 00:22:21 +0530\n        http://localhost:4000/cloud-ops/2019/hello-world/\n        http://localhost:4000/cloud-ops/2019/hello-world/\n        \n        \n        jekyll\n        \n        update\n        \n      \n    \n      \n        Welcome to Docsy Jekyll\n        &lt;p&gt;You’ll find this post in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;\n\n&lt;!--more--&gt;\n\n&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;\n\n&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;\n\n&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\n  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;\n&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;\n&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;Tom&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\n&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &apos;Hi, Tom&apos; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;\n\n&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;\n\n\n        Fri, 28 Jun 2019 16:22:21 +0530\n        http://localhost:4000/cloud-ops/2019/welcome/\n        http://localhost:4000/cloud-ops/2019/welcome/\n        \n        \n        jekyll\n        \n        update"
					}
					
				
		
				
					,
					
					"": {
						"id": "",
						"title": "",
						"categories": "",
						"url": " /",
						"content": ""
					}
					
				
		
				
					,
					
					"": {
						"id": "",
						"title": "CloudOps Documentation",
						"categories": "",
						"url": " /",
						"content": "Welcome to Cloud Ops team’s documentation pages\n\nPlease use the left menu to navigate through the site.\n\nFor quick searching, use the search bar on the left or use the tags page"
					}
					
				
		
				
		
				
					,
					
					"assets-js-main-js": {
						"id": "assets-js-main-js",
						"title": "",
						"categories": "",
						"url": " /assets/js/main.js",
						"content": "(function($) {\n    'use strict';\n    $(function() {\n        $('[data-toggle=\"tooltip\"]').tooltip();\n        $('[data-toggle=\"popover\"]').popover();\n        $('.popover-dismiss').popover({\n            trigger: 'focus'\n        })\n    });\n\n    function bottomPos(element) {\n        return element.offset().top + element.outerHeight();\n    }\n    $(function() {\n        var promo = $(\".js-td-cover\");\n        if (!promo.length) {\n            return\n        }\n        var promoOffset = bottomPos(promo);\n        var navbarOffset = $('.js-navbar-scroll').offset().top;\n        var threshold = Math.ceil($('.js-navbar-scroll').outerHeight());\n        if ((promoOffset - navbarOffset) < threshold) {\n            $('.js-navbar-scroll').addClass('navbar-bg-onscroll');\n        }\n        $(window).on('scroll', function() {\n            var navtop = $('.js-navbar-scroll').offset().top - $(window).scrollTop();\n            var promoOffset = bottomPos($('.js-td-cover'));\n            var navbarOffset = $('.js-navbar-scroll').offset().top;\n            if ((promoOffset - navbarOffset) < threshold) {\n                $('.js-navbar-scroll').addClass('navbar-bg-onscroll');\n            } else {\n                $('.js-navbar-scroll').removeClass('navbar-bg-onscroll');\n                $('.js-navbar-scroll').addClass('navbar-bg-onscroll--fade');\n            }\n        });\n    });\n}(jQuery));\n(function($) {\n    'use strict';\n    var Search = {\n        init: function() {\n            $(document).ready(function() {\n                $(document).on('keypress', '.td-search-input', function(e) {\n                    if (e.keyCode !== 13) {\n                        return\n                    }\n                    var query = $(this).val();\n                    var searchPage = \"http://localhost:4000/cloud-ops/search/?q=\" + query;\n                    document.location = searchPage;\n                    return false;\n                });\n            });\n        },\n    };\n    Search.init();\n}(jQuery));"
					}
					
				
		
				
					,
					
					"news": {
						"id": "news",
						"title": "News",
						"categories": "",
						"url": " /news/",
						"content": "News\n\nSubscribe with RSS to keep up with the latest news.\nFor site changes, see the changelog kept with the code base.\n\n\n\n\n   Two Thousand Nineteen\n   June 29, 2019\n   warning-badgedanger-badge\n   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n\n   \n   \n\n\n\n   Welcome to Docsy Jekyll\n   June 28, 2019\n   primary-badgesecondary-badgeinfo-badgesuccess-badge\n   You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\n\n\n   \n      read more\n   \n   \n\n\nWant to see more? See the News Archive."
					}
					
				
		
				
		
				
		
				
		
				
					,
					
					"sitemap-xml": {
						"id": "sitemap-xml",
						"title": "",
						"categories": "",
						"url": " /sitemap.xml",
						"content": "/\n     {{ \"now\" | date: \"%Y-%m-%d\" }}\n     daily\n    \n{% for section in site.data.toc %}\n     {{ site.baseurl }}{{ section.url }}/\n     {{ \"now\" | date: \"%Y-%m-%d\" }}\n     daily\n    \n{% endfor %}"
					}
					
				
		
				
					,
					
					"tags": {
						"id": "tags",
						"title": "Tags Index",
						"categories": "",
						"url": " /tags/",
						"content": "Tags Index\n{% capture site_tags %}{% for tag in site.tags %}{% if tag %}{{ tag | first }}{% unless forloop.last %},{% endunless %}{% endif %}{% endfor %}{% endcapture %}{% assign docs_tags = \"\" %}{% for doc in site.docs %}{% assign ttags = doc.tags | join:',' | append:',' %}{% assign docs_tags = docs_tags | append:ttags %}{% endfor %}\n{% assign all_tags = site_tags | append:docs_tags %}{% assign tags_list = all_tags | split:',' | uniq | sort %}\n\n{% for tag in tags_list %}{% if tag %}{{ tag }}\n\n    {% for post in site.tags[tag] %}\n    {{- post.title -}}\n     {{- post.date | date: \"%B %d, %Y\" -}}\n{%- endfor -%}\n{% for doc in site.docs %}{% if doc.tags contains tag %}\n\n    {{ doc.title }}\n         {{- doc.date | date: \"%B %d, %Y\" -}}\n    {% endif %}{% endfor %}\n{% endif %}{%- endfor -%}"
					}
					
				
		
				
					,
					
					"assets-css-style-css": {
						"id": "assets-css-style-css",
						"title": "",
						"categories": "",
						"url": " /assets/css/style.css",
						"content": "@import \"jekyll-theme-primer\";"
					}
					
				
		
	};
</script>
<script src="/cloud-ops/assets/js/lunr.min.js"></script>
<script src="/cloud-ops/assets/js/search.js"></script>

<script
  src="/cloud-ops/assets/js/jquery-3.3.1/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>

<script>
$(document).ready(function() {

    var toc = $('#TOC');

    // Select each header
    sections = $('.td-content h1');
        $.each(sections, function(idx, v) {
            section = $(v);
            var div_id = $(section).attr('id');
            var div_text = section.text().split('¶')[0];
            var parent = $("#" + div_id)
            var content = '<li id="link_' + div_id + '" class="md-nav__item"><a class="md-nav__link" href="#' + div_id + '" title="' + div_text +'">' + div_text +'</a></li>';
            $(toc).append(content);

            // Add section code to subnavigation
            var children = $('<nav class="md-nav"><ul class="md-nav__list"></nav></ul>')
            var contenders = $("#" + div_id).nextUntil("h1");
            $.each(contenders, function(idx, contender){
               if($(contender).is('h2') || $(contender).is('h3')) {
                   var contender_id = $(contender).attr('id');
                   var contender_text = $(contender).text().split('¶')[0];
                   var content = '<li class="md-nav__item"><a class="md-nav__link" href="#' + contender_id + '" title="' + contender_text +'">' + contender_text +'</a></li>';
                   children.append(content);
                }
             })
             $("#link_" + div_id).append(children);
        });
    });
</script>

<script>
var headers = ["h1", "h2", "h3", "h4"]
var colors = ["red", "orange", "green", "blue"]

$.each(headers, function(i, header){
    var color = colors[i];
    $(header).each(function () {
        var href=$(this).attr("id");
        $(this).append('<a class="headerlink" style="color:' + color + '" href="#' + href + '" title="Permanent link">¶</a>')
    });
})
</script>




              
              <!--<style>
  .feedback--answer {
    display: inline-block;
  }
  .feedback--answer-no {
    margin-left: 1em;
  }
  .feedback--response {
    display: none;
    margin-top: 1em;
  }
  .feedback--response__visible {
    display: block;
  }
</style>
<h5 class="feedback--title">Feedback</h5>
<p class="feedback--question">Was this page helpful?</p>
<button class="feedback--answer feedback--answer-yes">Yes</button>
<button class="feedback--answer feedback--answer-no">No</button>
<p class="feedback--response feedback--response-yes">
  Glad to hear it! Please <a href="https://memsql.atlassian.net/secure/CreateIssueDetails!init.jspa?pid=11934&issuetype=10005&summary=Title%20goes%20here&description=Description%20goes%20here&priority=3">tell us how we can improve</a>.
</p>
<p class="feedback--response feedback--response-no">
  Sorry to hear that. Please <a href="https://memsql.atlassian.net/secure/CreateIssueDetails!init.jspa?pid=11934&issuetype=10005&summary=Title%20goes%20here&description=Description%20goes%20here&priority=3">tell us how we can improve</a>.
</p>
<script>
  const yesButton = document.querySelector('.feedback--answer-yes');
  const noButton = document.querySelector('.feedback--answer-no');
  const yesResponse = document.querySelector('.feedback--response-yes');
  const noResponse = document.querySelector('.feedback--response-no');
  const disableButtons = () => {
    yesButton.disabled = true;
    noButton.disabled = true;
  };
  const sendFeedback = (value) => {
    if (typeof ga !== 'function') return;
    const args = {
      command: 'send',
      hitType: 'event',
      category: 'Helpful',
      action: 'click',
      label: window.location.pathname,
      value: value
    };
    ga(args.command, args.hitType, args.category, args.action, args.label, args.value);
  };
  yesButton.addEventListener('click', () => {
    yesResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(1);
  });
  noButton.addEventListener('click', () => {
    noResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(0);
  });
</script><br/>
-->
           </div>
          </main>
        </div>
      </div>
      <!--<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        <ul class="list-inline mb-0">  
          
            <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="" aria-label="Twitter" data-original-title="Twitter">
              <a class="text-white" target="_blank" href="https://twitter.com/vsoch">
                <i class="fab fa-twitter"></i>
              </a>
            </li>
          
          
            <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="" aria-label="LinkedIn" data-original-title="LinkedIn">
              <a class="text-white" target="_blank" href="https://linkedin.com/in/vsochat">
                <i class="fab fa-linkedin"></i>
              </a>
            </li>
          
        </ul>
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        <ul class="list-inline mb-0">  
          <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="" aria-label="GitHub" data-original-title="GitHub">
            <a class="text-white" target="_blank" href="https://github.com/vsoch/docsy-jekyll">
              <i class="fab fa-github"></i>
            </a>
          </li>
        </ul>
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">© 2022 Dinosaur Avocado All Rights Reserved</small>
        
        <p class="mt-2"><a href="/about/">About Docsy</a></p>	
      </div>
    </div>
  </div>
</footer>
-->
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<script src="/cloud-ops/assets/js/main.js"></script>

  </body>
</html>

